[Skip to main content](#__docusaurus_skipToContent_fallback)

**Help us build the JS tools that power AI apps at companies like Replit, Uber, LinkedIn, GitLab, and more. [Join our team!](https://jobs.ashbyhq.com/langchain/05efa205-8560-43fd-bfcc-3f7697561cfb?utm_source=https%3A%2F%2Fjs.langchain.com%2F&utm_campaign=langchainjs_docs)**

[![ü¶úÔ∏èüîó Langchain](/img/brand/wordmark.png)![ü¶úÔ∏èüîó Langchain](/img/brand/wordmark-dark.png)](/)[Integrations](/docs/integrations/platforms/)[API Reference](https://v03.api.js.langchain.com)

More

* [People](/docs/people/)
* [Community](/docs/community)
* [Error reference](/docs/troubleshooting/errors)
* [External guides](/docs/additional_resources/tutorials)
* [Contributing](/docs/contributing)

v0.3

* [v0.3](/docs/introduction)
* [v0.2](https://js.langchain.com/v0.2/docs/introduction)
* [v0.1](https://js.langchain.com/v0.1/docs/get_started/introduction)

ü¶úüîó

* [LangSmith](https://smith.langchain.com)
* [LangSmith Docs](https://docs.smith.langchain.com)
* [LangChain Hub](https://smith.langchain.com/hub)
* [LangServe](https://github.com/langchain-ai/langserve)
* [Python Docs](https://python.langchain.com/)

[Chat](https://chatjs.langchain.com)

Search

* [Introduction](/docs/introduction)
* [Tutorials](/docs/tutorials/)

  + [Build a Question Answering application over a Graph Database](/docs/tutorials/graph)
  + [Tutorials](/docs/tutorials/)
  + [Build a simple LLM application with chat models and prompt templates](/docs/tutorials/llm_chain)
  + [Build a Chatbot](/docs/tutorials/chatbot)
  + [Build a Retrieval Augmented Generation (RAG) App: Part 2](/docs/tutorials/qa_chat_history)
  + [Build an Extraction Chain](/docs/tutorials/extraction)
  + [Tagging](/docs/tutorials/classification)
  + [Build a Retrieval Augmented Generation (RAG) App: Part 1](/docs/tutorials/rag)
  + [Build a semantic search engine](/docs/tutorials/retrievers)
  + [Build a Question/Answering system over SQL data](/docs/tutorials/sql_qa)
  + [Summarize Text](/docs/tutorials/summarization)
* [How-to guides](/docs/how_to/)

  + [How-to guides](/docs/how_to/)
  + [How to add memory to chatbots](/docs/how_to/chatbots_memory)
  + [How to use example selectors](/docs/how_to/example_selectors)
  + [Installation](/docs/how_to/installation)
  + [How to stream responses from an LLM](/docs/how_to/streaming_llm)
  + [How to stream chat model responses](/docs/how_to/chat_streaming)
  + [How to embed text data](/docs/how_to/embed_text)
  + [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat)
  + [How to cache model responses](/docs/how_to/llm_caching)
  + [How to cache chat model responses](/docs/how_to/chat_model_caching)
  + [Richer outputs](/docs/how_to/custom_llm)
  + [How to use few shot examples](/docs/how_to/few_shot_examples)
  + [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)
  + [How to return structured data from a model](/docs/how_to/structured_output)
  + [How to add ad-hoc tool calling capability to LLMs and Chat Models](/docs/how_to/tools_prompting)
  + [Richer outputs](/docs/how_to/custom_chat)
  + [How to do per-user retrieval](/docs/how_to/qa_per_user)
  + [How to track token usage](/docs/how_to/chat_token_usage_tracking)
  + [How to track token usage](/docs/how_to/llm_token_usage_tracking)
  + [How to pass through arguments from one step to the next](/docs/how_to/passthrough)
  + [How to compose prompts together](/docs/how_to/prompts_composition)
  + [How to use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor)
  + [How to add values to a chain's state](/docs/how_to/assign)
  + [How to attach runtime arguments to a Runnable](/docs/how_to/binding)
  + [How to cache embedding results](/docs/how_to/caching_embeddings)
  + [How to attach callbacks to a module](/docs/how_to/callbacks_attach)
  + [How to pass callbacks into a module constructor](/docs/how_to/callbacks_constructor)
  + [How to dispatch custom callback events](/docs/how_to/callbacks_custom_events)
  + [How to pass callbacks in at runtime](/docs/how_to/callbacks_runtime)
  + [How to await callbacks in serverless environments](/docs/how_to/callbacks_serverless)
  + [How to cancel execution](/docs/how_to/cancel_execution)
  + [How to split by character](/docs/how_to/character_text_splitter)
  + [How to init any model in one line](/docs/how_to/chat_models_universal_init)
  + [How to do retrieval](/docs/how_to/chatbots_retrieval)
  + [How to add tools to chatbots](/docs/how_to/chatbots_tools)
  + [How to split code](/docs/how_to/code_splitter)
  + [How to do retrieval with contextual compression](/docs/how_to/contextual_compression)
  + [How to convert Runnables to Tools](/docs/how_to/convert_runnable_to_tool)
  + [How to create custom callback handlers](/docs/how_to/custom_callbacks)
  + [How to write a custom retriever class](/docs/how_to/custom_retriever)
  + [How to create Tools](/docs/how_to/custom_tools)
  + [How to debug your LLM apps](/docs/how_to/debugging)
  + [How to load CSV data](/docs/how_to/document_loader_csv)
  + [How to write a custom document loader](/docs/how_to/document_loader_custom)
  + [How to load data from a directory](/docs/how_to/document_loader_directory)
  + [How to load HTML](/docs/how_to/document_loader_html)
  + [How to load Markdown](/docs/how_to/document_loader_markdown)
  + [How to load PDF files](/docs/how_to/document_loader_pdf)
  + [How to load JSON data](/docs/how_to/document_loaders_json)
  + [How to combine results from multiple retrievers](/docs/how_to/ensemble_retriever)
  + [How to select examples from a LangSmith dataset](/docs/how_to/example_selectors_langsmith)
  + [How to select examples by length](/docs/how_to/example_selectors_length_based)
  + [How to select examples by similarity](/docs/how_to/example_selectors_similarity)
  + [How to use reference examples](/docs/how_to/extraction_examples)
  + [How to handle long text](/docs/how_to/extraction_long_text)
  + [How to do extraction without using function calling](/docs/how_to/extraction_parse)
  + [Fallbacks](/docs/how_to/fallbacks)
  + [Few Shot Prompt Templates](/docs/how_to/few_shot)
  + [How to filter messages](/docs/how_to/filter_messages)
  + [How to run custom functions](/docs/how_to/functions)
  + [How to build an LLM generated UI](/docs/how_to/generative_ui)
  + [How to construct knowledge graphs](/docs/how_to/graph_constructing)
  + [How to map values to a database](/docs/how_to/graph_mapping)
  + [How to improve results with prompting](/docs/how_to/graph_prompting)
  + [How to add a semantic layer over the database](/docs/how_to/graph_semantic)
  + [How to reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing)
  + [LangChain Expression Language Cheatsheet](/docs/how_to/lcel_cheatsheet)
  + [How to get log probabilities](/docs/how_to/logprobs)
  + [How to merge consecutive messages of the same type](/docs/how_to/merge_message_runs)
  + [How to add message history](/docs/how_to/message_history)
  + [How to migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent)
  + [How to generate multiple embeddings per document](/docs/how_to/multi_vector)
  + [How to pass multimodal data directly to models](/docs/how_to/multimodal_inputs)
  + [How to use multimodal prompts](/docs/how_to/multimodal_prompts)
  + [How to generate multiple queries to retrieve data for](/docs/how_to/multiple_queries)
  + [How to try to fix errors in output parsing](/docs/how_to/output_parser_fixing)
  + [How to parse JSON output](/docs/how_to/output_parser_json)
  + [How to parse XML output](/docs/how_to/output_parser_xml)
  + [How to invoke runnables in parallel](/docs/how_to/parallel)
  + [How to retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)
  + [How to partially format prompt templates](/docs/how_to/prompts_partial)
  + [How to add chat history](/docs/how_to/qa_chat_history_how_to)
  + [How to return citations](/docs/how_to/qa_citations)
  + [How to return sources](/docs/how_to/qa_sources)
  + [How to stream from a question-answering chain](/docs/how_to/qa_streaming)
  + [How to construct filters](/docs/how_to/query_constructing_filters)
  + [How to add examples to the prompt](/docs/how_to/query_few_shot)
  + [How to deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)
  + [How to handle multiple queries](/docs/how_to/query_multiple_queries)
  + [How to handle multiple retrievers](/docs/how_to/query_multiple_retrievers)
  + [How to handle cases where no queries are generated](/docs/how_to/query_no_queries)
  + [How to recursively split text by characters](/docs/how_to/recursive_text_splitter)
  + [How to reduce retrieval latency](/docs/how_to/reduce_retrieval_latency)
  + [How to route execution within a chain](/docs/how_to/routing)
  + [How to do "self-querying" retrieval](/docs/how_to/self_query)
  + [How to chain runnables](/docs/how_to/sequence)
  + [How to split text by tokens](/docs/how_to/split_by_token)
  + [How to deal with large databases](/docs/how_to/sql_large_db)
  + [How to use prompting to improve results](/docs/how_to/sql_prompting)
  + [How to do query validation](/docs/how_to/sql_query_checking)
  + [How to stream agent data to the client](/docs/how_to/stream_agent_client)
  + [How to stream structured output to the client](/docs/how_to/stream_tool_client)
  + [How to stream](/docs/how_to/streaming)
  + [How to create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)
  + [How to return artifacts from a tool](/docs/how_to/tool_artifacts)
  + [How to use chat models to call tools](/docs/how_to/tool_calling)
  + [How to disable parallel tool calling](/docs/how_to/tool_calling_parallel)
  + [How to call tools with multimodal data](/docs/how_to/tool_calls_multimodal)
  + [How to force tool calling behavior](/docs/how_to/tool_choice)
  + [How to access the RunnableConfig from a tool](/docs/how_to/tool_configure)
  + [How to pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model)
  + [How to pass run time values to tools](/docs/how_to/tool_runtime)
  + [How to stream events from a tool](/docs/how_to/tool_stream_events)
  + [How to stream tool calls](/docs/how_to/tool_streaming)
  + [How to use LangChain tools](/docs/how_to/tools_builtin)
  + [How to handle tool errors](/docs/how_to/tools_error)
  + [How to use few-shot prompting with tool calling](/docs/how_to/tools_few_shot)
  + [How to trim messages](/docs/how_to/trim_messages)
  + [How use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
  + [How to create and query vector stores](/docs/how_to/vectorstores)
* [Conceptual Guide](/docs/concepts/)

  + [Agents](/docs/concepts/agents)
  + [Architecture](/docs/concepts/architecture)
  + [Callbacks](/docs/concepts/callbacks)
  + [Chat history](/docs/concepts/chat_history)
  + [Chat models](/docs/concepts/chat_models)
  + [Document loaders](/docs/concepts/document_loaders)
  + [Embedding models](/docs/concepts/embedding_models)
  + [Evaluation](/docs/concepts/evaluation)
  + [Example selectors](/docs/concepts/example_selectors)
  + [Few-shot prompting](/docs/concepts/few_shot_prompting)
  + [Conceptual guide](/docs/concepts/)
  + [Key-value stores](/docs/concepts/key_value_stores)
  + [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
  + [Messages](/docs/concepts/messages)
  + [Multimodality](/docs/concepts/multimodality)
  + [Output parsers](/docs/concepts/output_parsers)
  + [Prompt Templates](/docs/concepts/prompt_templates)
  + [Retrieval augmented generation (rag)](/docs/concepts/rag)
  + [Retrieval](/docs/concepts/retrieval)
  + [Retrievers](/docs/concepts/retrievers)
  + [Runnable interface](/docs/concepts/runnables)
  + [Streaming](/docs/concepts/streaming)
  + [Structured outputs](/docs/concepts/structured_outputs)
  + [t](/docs/concepts/t)
  + [String-in, string-out llms](/docs/concepts/text_llms)
  + [Text splitters](/docs/concepts/text_splitters)
  + [Tokens](/docs/concepts/tokens)
  + [Tool calling](/docs/concepts/tool_calling)
  + [Tools](/docs/concepts/tools)
  + [Tracing](/docs/concepts/tracing)
  + [Vector stores](/docs/concepts/vectorstores)
  + [Why LangChain?](/docs/concepts/why_langchain)
* Ecosystem

  + [ü¶úüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/)
  + [ü¶úüï∏Ô∏è LangGraph.js](https://langchain-ai.github.io/langgraphjs/)
* Versions

  + [v0.3](/docs/versions/v0_3/)
  + [v0.2](/docs/versions/v0_2/)
  + [Migrating from v0.0 memory](/docs/versions/migrating_memory/)

    - [How to migrate to LangGraph memory](/docs/versions/migrating_memory/)
    - [How to use BaseChatMessageHistory with LangGraph](/docs/versions/migrating_memory/chat_history)
    - [Migrating off ConversationTokenBufferMemory](/docs/versions/migrating_memory/conversation_buffer_window_memory)
    - [Migrating off ConversationSummaryMemory or ConversationSummaryBufferMemory](/docs/versions/migrating_memory/conversation_summary_memory)
  + [Release Policy](/docs/versions/release_policy)
* [Security](/docs/security)

* [How-to guides](/docs/how_to/)
* How to init any model in one line

On this page

# How to init any model in one line

Many LLM applications let end users specify what model provider and model they want the application to be powered by.
This requires writing some logic to initialize different ChatModels based on some user configuration.
The `initChatModel()` helper method makes it easy to initialize a number of different model integrations without having to worry about import paths and class names.
Keep in mind this feature is only for chat models.

Prerequisites

This guide assumes familiarity with the following concepts:

* [Chat models](/docs/concepts/chat_models)
* [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
* [Tool calling](/docs/concepts/tools)

Compatibility

**This feature is only intended to be used in Node environments. Use in non Node environments or with bundlers is not guaranteed to work and not officially supported.**

`initChatModel` requires `langchain>=0.2.11`. See [this guide](/docs/how_to/installation/#installing-integration-packages) for some considerations to take when upgrading.

See the [initChatModel()](https://api.js.langchain.com/functions/langchain.chat_models_universal.initChatModel.html) API reference for a full list of supported integrations.

Make sure you have the integration packages installed for any model providers you want to support. E.g. you should have `@langchain/openai` installed to init an OpenAI model.

## Basic usage[‚Äã](#basic-usage "Direct link to Basic usage")

```
import { initChatModel } from "langchain/chat_models/universal";

// Returns a @langchain/openai ChatOpenAI instance.
const gpt4o = await initChatModel("gpt-4o", {
  modelProvider: "openai",
  temperature: 0,
});

// You can also specify the model provider in the model name like this in
// langchain>=0.3.18:

// Returns a @langchain/anthropic ChatAnthropic instance.
const claudeOpus = await initChatModel("anthropic:claude-3-opus-20240229", {
  temperature: 0,
});
// Returns a @langchain/google-vertexai ChatVertexAI instance.
const gemini15 = await initChatModel("google-vertexai:gemini-1.5-pro", {
  temperature: 0,
});

// Since all model integrations implement the ChatModel interface, you can use them in the same way.
console.log(`GPT-4o: ${(await gpt4o.invoke("what's your name")).content}\n`);
console.log(
  `Claude Opus: ${(await claudeOpus.invoke("what's your name")).content}\n`
);
console.log(
  `Gemini 1.5: ${(await gemini15.invoke("what's your name")).content}\n`
);

/*
GPT-4o: I'm an AI language model created by OpenAI, and I don't have a personal name. You can call me Assistant or any other name you prefer! How can I help you today?

Claude Opus: My name is Claude. It's nice to meet you!

Gemini 1.5: I don't have a name. I am a large language model, and I am not a person. I am a computer program that can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.
*/

```

#### API Reference:

* initChatModel from `langchain/chat_models/universal`

## Inferring model provider[‚Äã](#inferring-model-provider "Direct link to Inferring model provider")

For common and distinct model names `initChatModel()` will attempt to infer the model provider.
See the [API reference](https://api.js.langchain.com/functions/langchain.chat_models_universal.initChatModel.html) for a full list of inference behavior.
E.g. any model that starts with `gpt-3...` or `gpt-4...` will be inferred as using model provider `openai`.

```
import { initChatModel } from "langchain/chat_models/universal";

const gpt4o = await initChatModel("gpt-4o", {
  temperature: 0,
});
const claudeOpus = await initChatModel("claude-3-opus-20240229", {
  temperature: 0,
});
const gemini15 = await initChatModel("gemini-1.5-pro", {
  temperature: 0,
});

```

#### API Reference:

* initChatModel from `langchain/chat_models/universal`

## Creating a configurable model[‚Äã](#creating-a-configurable-model "Direct link to Creating a configurable model")

You can also create a runtime-configurable model by specifying `configurableFields`.
If you don't specify a `model` value, then "model" and "modelProvider" be configurable by default.

```
import { initChatModel } from "langchain/chat_models/universal";

const configurableModel = await initChatModel(undefined, { temperature: 0 });

const gpt4Res = await configurableModel.invoke("what's your name", {
  configurable: { model: "gpt-4o" },
});
console.log("gpt4Res: ", gpt4Res.content);
/*
gpt4Res:  I'm an AI language model created by OpenAI, and I don't have a personal name. You can call me Assistant or any other name you prefer! How can I assist you today?
*/

const claudeRes = await configurableModel.invoke("what's your name", {
  configurable: { model: "claude-3-5-sonnet-20240620" },
});
console.log("claudeRes: ", claudeRes.content);
/*
claudeRes:  My name is Claude. It's nice to meet you!
*/

```

#### API Reference:

* initChatModel from `langchain/chat_models/universal`

### Configurable model with default values[‚Äã](#configurable-model-with-default-values "Direct link to Configurable model with default values")

We can create a configurable model with default model values, specify which parameters are configurable, and add prefixes to configurable params:

```
import { initChatModel } from "langchain/chat_models/universal";

const firstLlm = await initChatModel("gpt-4o", {
  temperature: 0,
  configurableFields: ["model", "modelProvider", "temperature", "maxTokens"],
  configPrefix: "first", // useful when you have a chain with multiple models
});

const openaiRes = await firstLlm.invoke("what's your name");
console.log("openaiRes: ", openaiRes.content);
/*
openaiRes:  I'm an AI language model created by OpenAI, and I don't have a personal name. You can call me Assistant or any other name you prefer! How can I assist you today?
*/

const claudeRes = await firstLlm.invoke("what's your name", {
  configurable: {
    first_model: "claude-3-5-sonnet-20240620",
    first_temperature: 0.5,
    first_maxTokens: 100,
  },
});
console.log("claudeRes: ", claudeRes.content);
/*
claudeRes:  My name is Claude. It's nice to meet you!
*/

```

#### API Reference:

* initChatModel from `langchain/chat_models/universal`

### Using a configurable model declaratively[‚Äã](#using-a-configurable-model-declaratively "Direct link to Using a configurable model declaratively")

We can call declarative operations like `bindTools`, `withStructuredOutput`, `withConfig`, etc. on a configurable model and chain a configurable model in the same way that we would a regularly instantiated chat model object.

```
import { z } from "zod";
import { tool } from "@langchain/core/tools";
import { initChatModel } from "langchain/chat_models/universal";

const GetWeather = z
  .object({
    location: z.string().describe("The city and state, e.g. San Francisco, CA"),
  })
  .describe("Get the current weather in a given location");
const weatherTool = tool(
  (_) => {
    // do something
    return "138 degrees";
  },
  {
    name: "GetWeather",
    schema: GetWeather,
  }
);

const GetPopulation = z
  .object({
    location: z.string().describe("The city and state, e.g. San Francisco, CA"),
  })
  .describe("Get the current population in a given location");
const populationTool = tool(
  (_) => {
    // do something
    return "one hundred billion";
  },
  {
    name: "GetPopulation",
    schema: GetPopulation,
  }
);

const llm = await initChatModel(undefined, { temperature: 0 });
const llmWithTools = llm.bindTools([weatherTool, populationTool]);

const toolCalls1 = (
  await llmWithTools.invoke("what's bigger in 2024 LA or NYC", {
    configurable: { model: "gpt-4o" },
  })
).tool_calls;
console.log("toolCalls1: ", JSON.stringify(toolCalls1, null, 2));
/*
toolCalls1:  [
  {
    "name": "GetPopulation",
    "args": {
      "location": "Los Angeles, CA"
    },
    "type": "tool_call",
    "id": "call_DXRBVE4xfLYZfhZOsW1qRbr5"
  },
  {
    "name": "GetPopulation",
    "args": {
      "location": "New York, NY"
    },
    "type": "tool_call",
    "id": "call_6ec3m4eWhwGz97sCbNt7kOvC"
  }
]
*/

const toolCalls2 = (
  await llmWithTools.invoke("what's bigger in 2024 LA or NYC", {
    configurable: { model: "claude-3-5-sonnet-20240620" },
  })
).tool_calls;
console.log("toolCalls2: ", JSON.stringify(toolCalls2, null, 2));
/*
toolCalls2:  [
  {
    "name": "GetPopulation",
    "args": {
      "location": "Los Angeles, CA"
    },
    "id": "toolu_01K3jNU8jx18sJ9Y6Q9SooJ7",
    "type": "tool_call"
  },
  {
    "name": "GetPopulation",
    "args": {
      "location": "New York City, NY"
    },
    "id": "toolu_01UiANKaSwYykuF4hi3t5oNB",
    "type": "tool_call"
  }
]
*/

```

#### API Reference:

* tool from `@langchain/core/tools`
* initChatModel from `langchain/chat_models/universal`

---

#### Was this page helpful?

#### You can also leave detailed feedback [on GitHub](https://github.com/langchain-ai/langchainjs/issues/new?assignees=&labels=03+-+Documentation&projects=&template=documentation.yml&title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).

[Previous

How to split by character](/docs/how_to/character_text_splitter)[Next

How to do retrieval](/docs/how_to/chatbots_retrieval)

* [Basic usage](#basic-usage)
* [Inferring model provider](#inferring-model-provider)
* [Creating a configurable model](#creating-a-configurable-model)
  + [Configurable model with default values](#configurable-model-with-default-values)
  + [Using a configurable model declaratively](#using-a-configurable-model-declaratively)

Community

* [Twitter](https://twitter.com/LangChainAI)

GitHub

* [Python](https://github.com/langchain-ai/langchain)
* [JS/TS](https://github.com/langchain-ai/langchainjs)

More

* [Homepage](https://langchain.com)
* [Blog](https://blog.langchain.dev)

Copyright ¬© 2025 LangChain, Inc.
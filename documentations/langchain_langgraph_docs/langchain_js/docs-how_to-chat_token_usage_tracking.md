[Skip to main content](#__docusaurus_skipToContent_fallback)

**Help us build the JS tools that power AI apps at companies like Replit, Uber, LinkedIn, GitLab, and more. [Join our team!](https://jobs.ashbyhq.com/langchain/05efa205-8560-43fd-bfcc-3f7697561cfb?utm_source=https%3A%2F%2Fjs.langchain.com%2F&utm_campaign=langchainjs_docs)**

[![ðŸ¦œï¸ðŸ”— Langchain](/img/brand/wordmark.png)![ðŸ¦œï¸ðŸ”— Langchain](/img/brand/wordmark-dark.png)](/)[Integrations](/docs/integrations/platforms/)[API Reference](https://v03.api.js.langchain.com)

More

* [People](/docs/people/)
* [Community](/docs/community)
* [Error reference](/docs/troubleshooting/errors)
* [External guides](/docs/additional_resources/tutorials)
* [Contributing](/docs/contributing)

v0.3

* [v0.3](/docs/introduction)
* [v0.2](https://js.langchain.com/v0.2/docs/introduction)
* [v0.1](https://js.langchain.com/v0.1/docs/get_started/introduction)

ðŸ¦œðŸ”—

* [LangSmith](https://smith.langchain.com)
* [LangSmith Docs](https://docs.smith.langchain.com)
* [LangChain Hub](https://smith.langchain.com/hub)
* [LangServe](https://github.com/langchain-ai/langserve)
* [Python Docs](https://python.langchain.com/)

[Chat](https://chatjs.langchain.com)

Search

* [Introduction](/docs/introduction)
* [Tutorials](/docs/tutorials/)

  + [Build a Question Answering application over a Graph Database](/docs/tutorials/graph)
  + [Tutorials](/docs/tutorials/)
  + [Build a simple LLM application with chat models and prompt templates](/docs/tutorials/llm_chain)
  + [Build a Chatbot](/docs/tutorials/chatbot)
  + [Build a Retrieval Augmented Generation (RAG) App: Part 2](/docs/tutorials/qa_chat_history)
  + [Build an Extraction Chain](/docs/tutorials/extraction)
  + [Tagging](/docs/tutorials/classification)
  + [Build a Retrieval Augmented Generation (RAG) App: Part 1](/docs/tutorials/rag)
  + [Build a semantic search engine](/docs/tutorials/retrievers)
  + [Build a Question/Answering system over SQL data](/docs/tutorials/sql_qa)
  + [Summarize Text](/docs/tutorials/summarization)
* [How-to guides](/docs/how_to/)

  + [How-to guides](/docs/how_to/)
  + [How to add memory to chatbots](/docs/how_to/chatbots_memory)
  + [How to use example selectors](/docs/how_to/example_selectors)
  + [Installation](/docs/how_to/installation)
  + [How to stream responses from an LLM](/docs/how_to/streaming_llm)
  + [How to stream chat model responses](/docs/how_to/chat_streaming)
  + [How to embed text data](/docs/how_to/embed_text)
  + [How to use few shot examples in chat models](/docs/how_to/few_shot_examples_chat)
  + [How to cache model responses](/docs/how_to/llm_caching)
  + [How to cache chat model responses](/docs/how_to/chat_model_caching)
  + [Richer outputs](/docs/how_to/custom_llm)
  + [How to use few shot examples](/docs/how_to/few_shot_examples)
  + [How to use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)
  + [How to return structured data from a model](/docs/how_to/structured_output)
  + [How to add ad-hoc tool calling capability to LLMs and Chat Models](/docs/how_to/tools_prompting)
  + [Richer outputs](/docs/how_to/custom_chat)
  + [How to do per-user retrieval](/docs/how_to/qa_per_user)
  + [How to track token usage](/docs/how_to/chat_token_usage_tracking)
  + [How to track token usage](/docs/how_to/llm_token_usage_tracking)
  + [How to pass through arguments from one step to the next](/docs/how_to/passthrough)
  + [How to compose prompts together](/docs/how_to/prompts_composition)
  + [How to use legacy LangChain Agents (AgentExecutor)](/docs/how_to/agent_executor)
  + [How to add values to a chain's state](/docs/how_to/assign)
  + [How to attach runtime arguments to a Runnable](/docs/how_to/binding)
  + [How to cache embedding results](/docs/how_to/caching_embeddings)
  + [How to attach callbacks to a module](/docs/how_to/callbacks_attach)
  + [How to pass callbacks into a module constructor](/docs/how_to/callbacks_constructor)
  + [How to dispatch custom callback events](/docs/how_to/callbacks_custom_events)
  + [How to pass callbacks in at runtime](/docs/how_to/callbacks_runtime)
  + [How to await callbacks in serverless environments](/docs/how_to/callbacks_serverless)
  + [How to cancel execution](/docs/how_to/cancel_execution)
  + [How to split by character](/docs/how_to/character_text_splitter)
  + [How to init any model in one line](/docs/how_to/chat_models_universal_init)
  + [How to do retrieval](/docs/how_to/chatbots_retrieval)
  + [How to add tools to chatbots](/docs/how_to/chatbots_tools)
  + [How to split code](/docs/how_to/code_splitter)
  + [How to do retrieval with contextual compression](/docs/how_to/contextual_compression)
  + [How to convert Runnables to Tools](/docs/how_to/convert_runnable_to_tool)
  + [How to create custom callback handlers](/docs/how_to/custom_callbacks)
  + [How to write a custom retriever class](/docs/how_to/custom_retriever)
  + [How to create Tools](/docs/how_to/custom_tools)
  + [How to debug your LLM apps](/docs/how_to/debugging)
  + [How to load CSV data](/docs/how_to/document_loader_csv)
  + [How to write a custom document loader](/docs/how_to/document_loader_custom)
  + [How to load data from a directory](/docs/how_to/document_loader_directory)
  + [How to load HTML](/docs/how_to/document_loader_html)
  + [How to load Markdown](/docs/how_to/document_loader_markdown)
  + [How to load PDF files](/docs/how_to/document_loader_pdf)
  + [How to load JSON data](/docs/how_to/document_loaders_json)
  + [How to combine results from multiple retrievers](/docs/how_to/ensemble_retriever)
  + [How to select examples from a LangSmith dataset](/docs/how_to/example_selectors_langsmith)
  + [How to select examples by length](/docs/how_to/example_selectors_length_based)
  + [How to select examples by similarity](/docs/how_to/example_selectors_similarity)
  + [How to use reference examples](/docs/how_to/extraction_examples)
  + [How to handle long text](/docs/how_to/extraction_long_text)
  + [How to do extraction without using function calling](/docs/how_to/extraction_parse)
  + [Fallbacks](/docs/how_to/fallbacks)
  + [Few Shot Prompt Templates](/docs/how_to/few_shot)
  + [How to filter messages](/docs/how_to/filter_messages)
  + [How to run custom functions](/docs/how_to/functions)
  + [How to build an LLM generated UI](/docs/how_to/generative_ui)
  + [How to construct knowledge graphs](/docs/how_to/graph_constructing)
  + [How to map values to a database](/docs/how_to/graph_mapping)
  + [How to improve results with prompting](/docs/how_to/graph_prompting)
  + [How to add a semantic layer over the database](/docs/how_to/graph_semantic)
  + [How to reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how_to/indexing)
  + [LangChain Expression Language Cheatsheet](/docs/how_to/lcel_cheatsheet)
  + [How to get log probabilities](/docs/how_to/logprobs)
  + [How to merge consecutive messages of the same type](/docs/how_to/merge_message_runs)
  + [How to add message history](/docs/how_to/message_history)
  + [How to migrate from legacy LangChain agents to LangGraph](/docs/how_to/migrate_agent)
  + [How to generate multiple embeddings per document](/docs/how_to/multi_vector)
  + [How to pass multimodal data directly to models](/docs/how_to/multimodal_inputs)
  + [How to use multimodal prompts](/docs/how_to/multimodal_prompts)
  + [How to generate multiple queries to retrieve data for](/docs/how_to/multiple_queries)
  + [How to try to fix errors in output parsing](/docs/how_to/output_parser_fixing)
  + [How to parse JSON output](/docs/how_to/output_parser_json)
  + [How to parse XML output](/docs/how_to/output_parser_xml)
  + [How to invoke runnables in parallel](/docs/how_to/parallel)
  + [How to retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)
  + [How to partially format prompt templates](/docs/how_to/prompts_partial)
  + [How to add chat history](/docs/how_to/qa_chat_history_how_to)
  + [How to return citations](/docs/how_to/qa_citations)
  + [How to return sources](/docs/how_to/qa_sources)
  + [How to stream from a question-answering chain](/docs/how_to/qa_streaming)
  + [How to construct filters](/docs/how_to/query_constructing_filters)
  + [How to add examples to the prompt](/docs/how_to/query_few_shot)
  + [How to deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)
  + [How to handle multiple queries](/docs/how_to/query_multiple_queries)
  + [How to handle multiple retrievers](/docs/how_to/query_multiple_retrievers)
  + [How to handle cases where no queries are generated](/docs/how_to/query_no_queries)
  + [How to recursively split text by characters](/docs/how_to/recursive_text_splitter)
  + [How to reduce retrieval latency](/docs/how_to/reduce_retrieval_latency)
  + [How to route execution within a chain](/docs/how_to/routing)
  + [How to do "self-querying" retrieval](/docs/how_to/self_query)
  + [How to chain runnables](/docs/how_to/sequence)
  + [How to split text by tokens](/docs/how_to/split_by_token)
  + [How to deal with large databases](/docs/how_to/sql_large_db)
  + [How to use prompting to improve results](/docs/how_to/sql_prompting)
  + [How to do query validation](/docs/how_to/sql_query_checking)
  + [How to stream agent data to the client](/docs/how_to/stream_agent_client)
  + [How to stream structured output to the client](/docs/how_to/stream_tool_client)
  + [How to stream](/docs/how_to/streaming)
  + [How to create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)
  + [How to return artifacts from a tool](/docs/how_to/tool_artifacts)
  + [How to use chat models to call tools](/docs/how_to/tool_calling)
  + [How to disable parallel tool calling](/docs/how_to/tool_calling_parallel)
  + [How to call tools with multimodal data](/docs/how_to/tool_calls_multimodal)
  + [How to force tool calling behavior](/docs/how_to/tool_choice)
  + [How to access the RunnableConfig from a tool](/docs/how_to/tool_configure)
  + [How to pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model)
  + [How to pass run time values to tools](/docs/how_to/tool_runtime)
  + [How to stream events from a tool](/docs/how_to/tool_stream_events)
  + [How to stream tool calls](/docs/how_to/tool_streaming)
  + [How to use LangChain tools](/docs/how_to/tools_builtin)
  + [How to handle tool errors](/docs/how_to/tools_error)
  + [How to use few-shot prompting with tool calling](/docs/how_to/tools_few_shot)
  + [How to trim messages](/docs/how_to/trim_messages)
  + [How use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
  + [How to create and query vector stores](/docs/how_to/vectorstores)
* [Conceptual Guide](/docs/concepts/)

  + [Agents](/docs/concepts/agents)
  + [Architecture](/docs/concepts/architecture)
  + [Callbacks](/docs/concepts/callbacks)
  + [Chat history](/docs/concepts/chat_history)
  + [Chat models](/docs/concepts/chat_models)
  + [Document loaders](/docs/concepts/document_loaders)
  + [Embedding models](/docs/concepts/embedding_models)
  + [Evaluation](/docs/concepts/evaluation)
  + [Example selectors](/docs/concepts/example_selectors)
  + [Few-shot prompting](/docs/concepts/few_shot_prompting)
  + [Conceptual guide](/docs/concepts/)
  + [Key-value stores](/docs/concepts/key_value_stores)
  + [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
  + [Messages](/docs/concepts/messages)
  + [Multimodality](/docs/concepts/multimodality)
  + [Output parsers](/docs/concepts/output_parsers)
  + [Prompt Templates](/docs/concepts/prompt_templates)
  + [Retrieval augmented generation (rag)](/docs/concepts/rag)
  + [Retrieval](/docs/concepts/retrieval)
  + [Retrievers](/docs/concepts/retrievers)
  + [Runnable interface](/docs/concepts/runnables)
  + [Streaming](/docs/concepts/streaming)
  + [Structured outputs](/docs/concepts/structured_outputs)
  + [t](/docs/concepts/t)
  + [String-in, string-out llms](/docs/concepts/text_llms)
  + [Text splitters](/docs/concepts/text_splitters)
  + [Tokens](/docs/concepts/tokens)
  + [Tool calling](/docs/concepts/tool_calling)
  + [Tools](/docs/concepts/tools)
  + [Tracing](/docs/concepts/tracing)
  + [Vector stores](/docs/concepts/vectorstores)
  + [Why LangChain?](/docs/concepts/why_langchain)
* Ecosystem

  + [ðŸ¦œðŸ› ï¸ LangSmith](https://docs.smith.langchain.com/)
  + [ðŸ¦œðŸ•¸ï¸ LangGraph.js](https://langchain-ai.github.io/langgraphjs/)
* Versions

  + [v0.3](/docs/versions/v0_3/)
  + [v0.2](/docs/versions/v0_2/)
  + [Migrating from v0.0 memory](/docs/versions/migrating_memory/)

    - [How to migrate to LangGraph memory](/docs/versions/migrating_memory/)
    - [How to use BaseChatMessageHistory with LangGraph](/docs/versions/migrating_memory/chat_history)
    - [Migrating off ConversationTokenBufferMemory](/docs/versions/migrating_memory/conversation_buffer_window_memory)
    - [Migrating off ConversationSummaryMemory or ConversationSummaryBufferMemory](/docs/versions/migrating_memory/conversation_summary_memory)
  + [Release Policy](/docs/versions/release_policy)
* [Security](/docs/security)

* [How-to guides](/docs/how_to/)
* How to track token usage

On this page

# How to track token usage

Prerequisites

This guide assumes familiarity with the following concepts:

* [Chat models](/docs/concepts/chat_models)

This notebook goes over how to track your token usage for specific calls.

## Using `AIMessage.usage_metadata`[â€‹](#using-aimessageusage_metadata "Direct link to using-aimessageusage_metadata")

A number of model providers return token usage information as part of the chat generation response. When available, this information will be included on the `AIMessage` objects produced by the corresponding model.

LangChain `AIMessage` objects include a [`usage_metadata`](https://api.js.langchain.com/classes/langchain_core.messages.AIMessage.html#usage_metadata) attribute for supported providers. When populated, this attribute will be an object with standard keys (e.g., "input\_tokens" and "output\_tokens").

#### OpenAI[â€‹](#openai "Direct link to OpenAI")

tip

See [this section for general instructions on installing integration packages](/docs/how_to/installation#installing-integration-packages).

* npm
* Yarn
* pnpm

```
npm install @langchain/openai @langchain/core

```

```
yarn add @langchain/openai @langchain/core

```

```
pnpm add @langchain/openai @langchain/core

```

```
import { ChatOpenAI } from "@langchain/openai";

const chatModel = new ChatOpenAI({
  model: "gpt-3.5-turbo-0125",
});

const res = await chatModel.invoke("Tell me a joke.");

console.log(res.usage_metadata);

/*
  { input_tokens: 12, output_tokens: 17, total_tokens: 29 }
*/

```

#### API Reference:

* ChatOpenAI from `@langchain/openai`

#### Anthropic[â€‹](#anthropic "Direct link to Anthropic")

* npm
* Yarn
* pnpm

```
npm install @langchain/anthropic @langchain/core

```

```
yarn add @langchain/anthropic @langchain/core

```

```
pnpm add @langchain/anthropic @langchain/core

```

```
import { ChatAnthropic } from "@langchain/anthropic";

const chatModel = new ChatAnthropic({
  model: "claude-3-haiku-20240307",
});

const res = await chatModel.invoke("Tell me a joke.");

console.log(res.usage_metadata);

/*
  { input_tokens: 12, output_tokens: 98, total_tokens: 110 }
*/

```

#### API Reference:

* ChatAnthropic from `@langchain/anthropic`

## Using `AIMessage.response_metadata`[â€‹](#using-aimessageresponse_metadata "Direct link to using-aimessageresponse_metadata")

A number of model providers return token usage information as part of the chat generation response. When available, this is included in the `AIMessage.response_metadata` field.

#### OpenAI[â€‹](#openai-1 "Direct link to OpenAI")

```
import { ChatOpenAI } from "@langchain/openai";

const chatModel = new ChatOpenAI({
  model: "gpt-4o-mini",
});

const res = await chatModel.invoke("Tell me a joke.");

console.log(res.response_metadata);

/*
  {
    tokenUsage: { completionTokens: 15, promptTokens: 12, totalTokens: 27 },
    finish_reason: 'stop'
  }
*/

```

#### API Reference:

* ChatOpenAI from `@langchain/openai`

#### Anthropic[â€‹](#anthropic-1 "Direct link to Anthropic")

```
import { ChatAnthropic } from "@langchain/anthropic";

const chatModel = new ChatAnthropic({
  model: "claude-3-sonnet-20240229",
});

const res = await chatModel.invoke("Tell me a joke.");

console.log(res.response_metadata);

/*
  {
    id: 'msg_017Mgz6HdgNbi3cwL1LNB9Dw',
    model: 'claude-3-sonnet-20240229',
    stop_sequence: null,
    usage: { input_tokens: 12, output_tokens: 30 },
    stop_reason: 'end_turn'
  }
*/

```

#### API Reference:

* ChatAnthropic from `@langchain/anthropic`

## Streaming[â€‹](#streaming "Direct link to Streaming")

Some providers support token count metadata in a streaming context.

#### OpenAI[â€‹](#openai-2 "Direct link to OpenAI")

For example, OpenAI will return a message chunk at the end of a stream with token usage information. This behavior is supported by `@langchain/openai` >= 0.1.0 and can be enabled by passing a `stream_options` parameter when making your call.

info

By default, the last message chunk in a stream will include a `finish_reason` in the message's `response_metadata` attribute. If we include token usage in streaming mode, an additional chunk containing usage metadata will be added to the end of the stream, such that `finish_reason` appears on the second to last message chunk.

```
import type { AIMessageChunk } from "@langchain/core/messages";
import { ChatOpenAI } from "@langchain/openai";
import { concat } from "@langchain/core/utils/stream";

// Instantiate the model
const model = new ChatOpenAI();

const response = await model.stream("Hello, how are you?", {
  // Pass the stream options
  stream_options: {
    include_usage: true,
  },
});

// Iterate over the response, only saving the last chunk
let finalResult: AIMessageChunk | undefined;
for await (const chunk of response) {
  if (finalResult) {
    finalResult = concat(finalResult, chunk);
  } else {
    finalResult = chunk;
  }
}

console.log(finalResult?.usage_metadata);

/*
  { input_tokens: 13, output_tokens: 30, total_tokens: 43 }
*/

```

#### API Reference:

* AIMessageChunk from `@langchain/core/messages`
* ChatOpenAI from `@langchain/openai`
* concat from `@langchain/core/utils/stream`

## Using callbacks[â€‹](#using-callbacks "Direct link to Using callbacks")

You can also use the `handleLLMEnd` callback to get the full output from the LLM, including token usage for supported models.
Here's an example of how you could do that:

```
import { ChatOpenAI } from "@langchain/openai";

const chatModel = new ChatOpenAI({
  model: "gpt-4o-mini",
  callbacks: [
    {
      handleLLMEnd(output) {
        console.log(JSON.stringify(output, null, 2));
      },
    },
  ],
});

await chatModel.invoke("Tell me a joke.");

/*
  {
    "generations": [
      [
        {
          "text": "Why did the scarecrow win an award?\n\nBecause he was outstanding in his field!",
          "message": {
            "lc": 1,
            "type": "constructor",
            "id": [
              "langchain_core",
              "messages",
              "AIMessage"
            ],
            "kwargs": {
              "content": "Why did the scarecrow win an award?\n\nBecause he was outstanding in his field!",
              "tool_calls": [],
              "invalid_tool_calls": [],
              "additional_kwargs": {},
              "response_metadata": {
                "tokenUsage": {
                  "completionTokens": 17,
                  "promptTokens": 12,
                  "totalTokens": 29
                },
                "finish_reason": "stop"
              }
            }
          },
          "generationInfo": {
            "finish_reason": "stop"
          }
        }
      ]
    ],
    "llmOutput": {
      "tokenUsage": {
        "completionTokens": 17,
        "promptTokens": 12,
        "totalTokens": 29
      }
    }
  }
*/

```

#### API Reference:

* ChatOpenAI from `@langchain/openai`

## Next steps[â€‹](#next-steps "Direct link to Next steps")

You've now seen a few examples of how to track chat model token usage for supported providers.

Next, check out the other how-to guides on chat models in this section, like [how to get a model to return structured output](/docs/how_to/structured_output) or [how to add caching to your chat models](/docs/how_to/chat_model_caching).

---

#### Was this page helpful?

#### You can also leave detailed feedback [on GitHub](https://github.com/langchain-ai/langchainjs/issues/new?assignees=&labels=03+-+Documentation&projects=&template=documentation.yml&title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).

[Previous

How to do per-user retrieval](/docs/how_to/qa_per_user)[Next

How to track token usage](/docs/how_to/llm_token_usage_tracking)

* [Using `AIMessage.usage_metadata`](#using-aimessageusage_metadata)
* [Using `AIMessage.response_metadata`](#using-aimessageresponse_metadata)
* [Streaming](#streaming)
* [Using callbacks](#using-callbacks)
* [Next steps](#next-steps)

Community

* [Twitter](https://twitter.com/LangChainAI)

GitHub

* [Python](https://github.com/langchain-ai/langchain)
* [JS/TS](https://github.com/langchain-ai/langchainjs)

More

* [Homepage](https://langchain.com)
* [Blog](https://blog.langchain.dev)

Copyright Â© 2025 LangChain, Inc.
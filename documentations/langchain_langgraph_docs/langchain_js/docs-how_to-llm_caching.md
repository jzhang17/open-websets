[Skip to main content](#%5F%5Fdocusaurus%5FskipToContent%5Ffallback)

**Help us build the JS tools that power AI apps at companies like Replit, Uber, LinkedIn, GitLab, and more. [Join our team!](https://jobs.ashbyhq.com/langchain/05efa205-8560-43fd-bfcc-3f7697561cfb?utm%5Fsource=https%3A%2F%2Fjs.langchain.com%2F&utm%5Fcampaign=langchainjs%5Fdocs)**

[![ü¶úÔ∏èüîó Langchain](/img/brand/wordmark.png)![ü¶úÔ∏èüîó Langchain](/img/brand/wordmark-dark.png)](/)[Integrations](/docs/integrations/platforms/)[API Reference](https://v03.api.js.langchain.com)

[More](#)
* [People](/docs/people/)
* [Community](/docs/community)
* [Error reference](/docs/troubleshooting/errors)
* [External guides](/docs/additional%5Fresources/tutorials)
* [Contributing](/docs/contributing)

[v0.3](#)
* [v0.3](/docs/introduction)
* [v0.2](https://js.langchain.com/v0.2/docs/introduction)
* [v0.1](https://js.langchain.com/v0.1/docs/get%5Fstarted/introduction)

[ü¶úüîó](#)
* [LangSmith](https://smith.langchain.com)
* [LangSmith Docs](https://docs.smith.langchain.com)
* [LangChain Hub](https://smith.langchain.com/hub)
* [LangServe](https://github.com/langchain-ai/langserve)
* [Python Docs](https://python.langchain.com/)

[Chat](https://chatjs.langchain.com)[](https://github.com/langchain-ai/langchainjs)

Search

* [Introduction](/docs/introduction)
* [Tutorials](/docs/tutorials/)  
   * [Build a Question Answering application over a Graph Database](/docs/tutorials/graph)  
   * [Tutorials](/docs/tutorials/)  
   * [Build a simple LLM application with chat models and prompt templates](/docs/tutorials/llm%5Fchain)  
   * [Build a Chatbot](/docs/tutorials/chatbot)  
   * [Build a Retrieval Augmented Generation (RAG) App: Part 2](/docs/tutorials/qa%5Fchat%5Fhistory)  
   * [Build an Extraction Chain](/docs/tutorials/extraction)  
   * [Tagging](/docs/tutorials/classification)  
   * [Build a Retrieval Augmented Generation (RAG) App: Part 1](/docs/tutorials/rag)  
   * [Build a semantic search engine](/docs/tutorials/retrievers)  
   * [Build a Question/Answering system over SQL data](/docs/tutorials/sql%5Fqa)  
   * [Summarize Text](/docs/tutorials/summarization)
* [How-to guides](/docs/how%5Fto/)  
   * [How-to guides](/docs/how%5Fto/)  
   * [How to add memory to chatbots](/docs/how%5Fto/chatbots%5Fmemory)  
   * [How to use example selectors](/docs/how%5Fto/example%5Fselectors)  
   * [Installation](/docs/how%5Fto/installation)  
   * [How to stream responses from an LLM](/docs/how%5Fto/streaming%5Fllm)  
   * [How to stream chat model responses](/docs/how%5Fto/chat%5Fstreaming)  
   * [How to embed text data](/docs/how%5Fto/embed%5Ftext)  
   * [How to use few shot examples in chat models](/docs/how%5Fto/few%5Fshot%5Fexamples%5Fchat)  
   * [How to cache model responses](/docs/how%5Fto/llm%5Fcaching)  
   * [How to cache chat model responses](/docs/how%5Fto/chat%5Fmodel%5Fcaching)  
   * [Richer outputs](/docs/how%5Fto/custom%5Fllm)  
   * [How to use few shot examples](/docs/how%5Fto/few%5Fshot%5Fexamples)  
   * [How to use output parsers to parse an LLM response into structured format](/docs/how%5Fto/output%5Fparser%5Fstructured)  
   * [How to return structured data from a model](/docs/how%5Fto/structured%5Foutput)  
   * [How to add ad-hoc tool calling capability to LLMs and Chat Models](/docs/how%5Fto/tools%5Fprompting)  
   * [Richer outputs](/docs/how%5Fto/custom%5Fchat)  
   * [How to do per-user retrieval](/docs/how%5Fto/qa%5Fper%5Fuser)  
   * [How to track token usage](/docs/how%5Fto/chat%5Ftoken%5Fusage%5Ftracking)  
   * [How to track token usage](/docs/how%5Fto/llm%5Ftoken%5Fusage%5Ftracking)  
   * [How to pass through arguments from one step to the next](/docs/how%5Fto/passthrough)  
   * [How to compose prompts together](/docs/how%5Fto/prompts%5Fcomposition)  
   * [How to use legacy LangChain Agents (AgentExecutor)](/docs/how%5Fto/agent%5Fexecutor)  
   * [How to add values to a chain's state](/docs/how%5Fto/assign)  
   * [How to attach runtime arguments to a Runnable](/docs/how%5Fto/binding)  
   * [How to cache embedding results](/docs/how%5Fto/caching%5Fembeddings)  
   * [How to attach callbacks to a module](/docs/how%5Fto/callbacks%5Fattach)  
   * [How to pass callbacks into a module constructor](/docs/how%5Fto/callbacks%5Fconstructor)  
   * [How to dispatch custom callback events](/docs/how%5Fto/callbacks%5Fcustom%5Fevents)  
   * [How to pass callbacks in at runtime](/docs/how%5Fto/callbacks%5Fruntime)  
   * [How to await callbacks in serverless environments](/docs/how%5Fto/callbacks%5Fserverless)  
   * [How to cancel execution](/docs/how%5Fto/cancel%5Fexecution)  
   * [How to split by character](/docs/how%5Fto/character%5Ftext%5Fsplitter)  
   * [How to init any model in one line](/docs/how%5Fto/chat%5Fmodels%5Funiversal%5Finit)  
   * [How to do retrieval](/docs/how%5Fto/chatbots%5Fretrieval)  
   * [How to add tools to chatbots](/docs/how%5Fto/chatbots%5Ftools)  
   * [How to split code](/docs/how%5Fto/code%5Fsplitter)  
   * [How to do retrieval with contextual compression](/docs/how%5Fto/contextual%5Fcompression)  
   * [How to convert Runnables to Tools](/docs/how%5Fto/convert%5Frunnable%5Fto%5Ftool)  
   * [How to create custom callback handlers](/docs/how%5Fto/custom%5Fcallbacks)  
   * [How to write a custom retriever class](/docs/how%5Fto/custom%5Fretriever)  
   * [How to create Tools](/docs/how%5Fto/custom%5Ftools)  
   * [How to debug your LLM apps](/docs/how%5Fto/debugging)  
   * [How to load CSV data](/docs/how%5Fto/document%5Floader%5Fcsv)  
   * [How to write a custom document loader](/docs/how%5Fto/document%5Floader%5Fcustom)  
   * [How to load data from a directory](/docs/how%5Fto/document%5Floader%5Fdirectory)  
   * [How to load HTML](/docs/how%5Fto/document%5Floader%5Fhtml)  
   * [How to load Markdown](/docs/how%5Fto/document%5Floader%5Fmarkdown)  
   * [How to load PDF files](/docs/how%5Fto/document%5Floader%5Fpdf)  
   * [How to load JSON data](/docs/how%5Fto/document%5Floaders%5Fjson)  
   * [How to combine results from multiple retrievers](/docs/how%5Fto/ensemble%5Fretriever)  
   * [How to select examples from a LangSmith dataset](/docs/how%5Fto/example%5Fselectors%5Flangsmith)  
   * [How to select examples by length](/docs/how%5Fto/example%5Fselectors%5Flength%5Fbased)  
   * [How to select examples by similarity](/docs/how%5Fto/example%5Fselectors%5Fsimilarity)  
   * [How to use reference examples](/docs/how%5Fto/extraction%5Fexamples)  
   * [How to handle long text](/docs/how%5Fto/extraction%5Flong%5Ftext)  
   * [How to do extraction without using function calling](/docs/how%5Fto/extraction%5Fparse)  
   * [Fallbacks](/docs/how%5Fto/fallbacks)  
   * [Few Shot Prompt Templates](/docs/how%5Fto/few%5Fshot)  
   * [How to filter messages](/docs/how%5Fto/filter%5Fmessages)  
   * [How to run custom functions](/docs/how%5Fto/functions)  
   * [How to build an LLM generated UI](/docs/how%5Fto/generative%5Fui)  
   * [How to construct knowledge graphs](/docs/how%5Fto/graph%5Fconstructing)  
   * [How to map values to a database](/docs/how%5Fto/graph%5Fmapping)  
   * [How to improve results with prompting](/docs/how%5Fto/graph%5Fprompting)  
   * [How to add a semantic layer over the database](/docs/how%5Fto/graph%5Fsemantic)  
   * [How to reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how%5Fto/indexing)  
   * [LangChain Expression Language Cheatsheet](/docs/how%5Fto/lcel%5Fcheatsheet)  
   * [How to get log probabilities](/docs/how%5Fto/logprobs)  
   * [How to merge consecutive messages of the same type](/docs/how%5Fto/merge%5Fmessage%5Fruns)  
   * [How to add message history](/docs/how%5Fto/message%5Fhistory)  
   * [How to migrate from legacy LangChain agents to LangGraph](/docs/how%5Fto/migrate%5Fagent)  
   * [How to generate multiple embeddings per document](/docs/how%5Fto/multi%5Fvector)  
   * [How to pass multimodal data directly to models](/docs/how%5Fto/multimodal%5Finputs)  
   * [How to use multimodal prompts](/docs/how%5Fto/multimodal%5Fprompts)  
   * [How to generate multiple queries to retrieve data for](/docs/how%5Fto/multiple%5Fqueries)  
   * [How to try to fix errors in output parsing](/docs/how%5Fto/output%5Fparser%5Ffixing)  
   * [How to parse JSON output](/docs/how%5Fto/output%5Fparser%5Fjson)  
   * [How to parse XML output](/docs/how%5Fto/output%5Fparser%5Fxml)  
   * [How to invoke runnables in parallel](/docs/how%5Fto/parallel)  
   * [How to retrieve the whole document for a chunk](/docs/how%5Fto/parent%5Fdocument%5Fretriever)  
   * [How to partially format prompt templates](/docs/how%5Fto/prompts%5Fpartial)  
   * [How to add chat history](/docs/how%5Fto/qa%5Fchat%5Fhistory%5Fhow%5Fto)  
   * [How to return citations](/docs/how%5Fto/qa%5Fcitations)  
   * [How to return sources](/docs/how%5Fto/qa%5Fsources)  
   * [How to stream from a question-answering chain](/docs/how%5Fto/qa%5Fstreaming)  
   * [How to construct filters](/docs/how%5Fto/query%5Fconstructing%5Ffilters)  
   * [How to add examples to the prompt](/docs/how%5Fto/query%5Ffew%5Fshot)  
   * [How to deal with high cardinality categorical variables](/docs/how%5Fto/query%5Fhigh%5Fcardinality)  
   * [How to handle multiple queries](/docs/how%5Fto/query%5Fmultiple%5Fqueries)  
   * [How to handle multiple retrievers](/docs/how%5Fto/query%5Fmultiple%5Fretrievers)  
   * [How to handle cases where no queries are generated](/docs/how%5Fto/query%5Fno%5Fqueries)  
   * [How to recursively split text by characters](/docs/how%5Fto/recursive%5Ftext%5Fsplitter)  
   * [How to reduce retrieval latency](/docs/how%5Fto/reduce%5Fretrieval%5Flatency)  
   * [How to route execution within a chain](/docs/how%5Fto/routing)  
   * [How to do "self-querying" retrieval](/docs/how%5Fto/self%5Fquery)  
   * [How to chain runnables](/docs/how%5Fto/sequence)  
   * [How to split text by tokens](/docs/how%5Fto/split%5Fby%5Ftoken)  
   * [How to deal with large databases](/docs/how%5Fto/sql%5Flarge%5Fdb)  
   * [How to use prompting to improve results](/docs/how%5Fto/sql%5Fprompting)  
   * [How to do query validation](/docs/how%5Fto/sql%5Fquery%5Fchecking)  
   * [How to stream agent data to the client](/docs/how%5Fto/stream%5Fagent%5Fclient)  
   * [How to stream structured output to the client](/docs/how%5Fto/stream%5Ftool%5Fclient)  
   * [How to stream](/docs/how%5Fto/streaming)  
   * [How to create a time-weighted retriever](/docs/how%5Fto/time%5Fweighted%5Fvectorstore)  
   * [How to return artifacts from a tool](/docs/how%5Fto/tool%5Fartifacts)  
   * [How to use chat models to call tools](/docs/how%5Fto/tool%5Fcalling)  
   * [How to disable parallel tool calling](/docs/how%5Fto/tool%5Fcalling%5Fparallel)  
   * [How to call tools with multimodal data](/docs/how%5Fto/tool%5Fcalls%5Fmultimodal)  
   * [How to force tool calling behavior](/docs/how%5Fto/tool%5Fchoice)  
   * [How to access the RunnableConfig from a tool](/docs/how%5Fto/tool%5Fconfigure)  
   * [How to pass tool outputs to chat models](/docs/how%5Fto/tool%5Fresults%5Fpass%5Fto%5Fmodel)  
   * [How to pass run time values to tools](/docs/how%5Fto/tool%5Fruntime)  
   * [How to stream events from a tool](/docs/how%5Fto/tool%5Fstream%5Fevents)  
   * [How to stream tool calls](/docs/how%5Fto/tool%5Fstreaming)  
   * [How to use LangChain tools](/docs/how%5Fto/tools%5Fbuiltin)  
   * [How to handle tool errors](/docs/how%5Fto/tools%5Ferror)  
   * [How to use few-shot prompting with tool calling](/docs/how%5Fto/tools%5Ffew%5Fshot)  
   * [How to trim messages](/docs/how%5Fto/trim%5Fmessages)  
   * [How use a vector store to retrieve data](/docs/how%5Fto/vectorstore%5Fretriever)  
   * [How to create and query vector stores](/docs/how%5Fto/vectorstores)
* [Conceptual Guide](/docs/concepts/)  
   * [Agents](/docs/concepts/agents)  
   * [Architecture](/docs/concepts/architecture)  
   * [Callbacks](/docs/concepts/callbacks)  
   * [Chat history](/docs/concepts/chat%5Fhistory)  
   * [Chat models](/docs/concepts/chat%5Fmodels)  
   * [Document loaders](/docs/concepts/document%5Floaders)  
   * [Embedding models](/docs/concepts/embedding%5Fmodels)  
   * [Evaluation](/docs/concepts/evaluation)  
   * [Example selectors](/docs/concepts/example%5Fselectors)  
   * [Few-shot prompting](/docs/concepts/few%5Fshot%5Fprompting)  
   * [Conceptual guide](/docs/concepts/)  
   * [Key-value stores](/docs/concepts/key%5Fvalue%5Fstores)  
   * [LangChain Expression Language (LCEL)](/docs/concepts/lcel)  
   * [Messages](/docs/concepts/messages)  
   * [Multimodality](/docs/concepts/multimodality)  
   * [Output parsers](/docs/concepts/output%5Fparsers)  
   * [Prompt Templates](/docs/concepts/prompt%5Ftemplates)  
   * [Retrieval augmented generation (rag)](/docs/concepts/rag)  
   * [Retrieval](/docs/concepts/retrieval)  
   * [Retrievers](/docs/concepts/retrievers)  
   * [Runnable interface](/docs/concepts/runnables)  
   * [Streaming](/docs/concepts/streaming)  
   * [Structured outputs](/docs/concepts/structured%5Foutputs)  
   * [t](/docs/concepts/t)  
   * [String-in, string-out llms](/docs/concepts/text%5Fllms)  
   * [Text splitters](/docs/concepts/text%5Fsplitters)  
   * [Tokens](/docs/concepts/tokens)  
   * [Tool calling](/docs/concepts/tool%5Fcalling)  
   * [Tools](/docs/concepts/tools)  
   * [Tracing](/docs/concepts/tracing)  
   * [Vector stores](/docs/concepts/vectorstores)  
   * [Why LangChain?](/docs/concepts/why%5Flangchain)
* Ecosystem  
   * [ü¶úüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/)  
   * [ü¶úüï∏Ô∏è LangGraph.js](https://langchain-ai.github.io/langgraphjs/)
* Versions  
   * [v0.3](/docs/versions/v0%5F3/)  
   * [v0.2](/docs/versions/v0%5F2/)  
   * [Migrating from v0.0 memory](/docs/versions/migrating%5Fmemory/)  
         * [How to migrate to LangGraph memory](/docs/versions/migrating%5Fmemory/)  
         * [How to use BaseChatMessageHistory with LangGraph](/docs/versions/migrating%5Fmemory/chat%5Fhistory)  
         * [Migrating off ConversationTokenBufferMemory](/docs/versions/migrating%5Fmemory/conversation%5Fbuffer%5Fwindow%5Fmemory)  
         * [Migrating off ConversationSummaryMemory or ConversationSummaryBufferMemory](/docs/versions/migrating%5Fmemory/conversation%5Fsummary%5Fmemory)  
   * [Release Policy](/docs/versions/release%5Fpolicy)
* [Security](/docs/security)

* [How-to guides](/docs/how%5Fto/)
* How to cache model responses

On this page

# How to cache model responses

LangChain provides an optional caching layer for LLMs. This is useful for two reasons:

It can save you money by reducing the number of API calls you make to the LLM provider, if you're often requesting the same completion multiple times. It can speed up your application by reducing the number of API calls you make to the LLM provider.

tip

See [this section for general instructions on installing integration packages](/docs/how%5Fto/installation#installing-integration-packages).

* npm
* Yarn
* pnpm

```
npm install @langchain/openai @langchain/core

```

```
yarn add @langchain/openai @langchain/core

```

```
pnpm add @langchain/openai @langchain/core

```

```
import { OpenAI } from "@langchain/openai";

const model = new OpenAI({
  model: "gpt-3.5-turbo-instruct",
  cache: true,
});

```

## In Memory Cache[‚Äã](#in-memory-cache "Direct link to In Memory Cache")

The default cache is stored in-memory. This means that if you restart your application, the cache will be cleared.

```
console.time();

// The first time, it is not yet in cache, so it should take longer
const res = await model.invoke("Tell me a long joke");

console.log(res);

console.timeEnd();

/*
  A man walks into a bar and sees a jar filled with money on the counter. Curious, he asks the bartender about it.

  The bartender explains, "We have a challenge for our customers. If you can complete three tasks, you win all the money in the jar."

  Intrigued, the man asks what the tasks are.

  The bartender replies, "First, you have to drink a whole bottle of tequila without making a face. Second, there's a pitbull out back with a sore tooth. You have to pull it out. And third, there's an old lady upstairs who has never had an orgasm. You have to give her one."

  The man thinks for a moment and then confidently says, "I'll do it."

  He grabs the bottle of tequila and downs it in one gulp, without flinching. He then heads to the back and after a few minutes of struggling, emerges with the pitbull's tooth in hand.

  The bar erupts in cheers and the bartender leads the man upstairs to the old lady's room. After a few minutes, the man walks out with a big smile on his face and the old lady is giggling with delight.

  The bartender hands the man the jar of money and asks, "How

  default: 4.187s
*/

```

```
console.time();

// The second time it is, so it goes faster
const res2 = await model.invoke("Tell me a joke");

console.log(res2);

console.timeEnd();

/*
  A man walks into a bar and sees a jar filled with money on the counter. Curious, he asks the bartender about it.

  The bartender explains, "We have a challenge for our customers. If you can complete three tasks, you win all the money in the jar."

  Intrigued, the man asks what the tasks are.

  The bartender replies, "First, you have to drink a whole bottle of tequila without making a face. Second, there's a pitbull out back with a sore tooth. You have to pull it out. And third, there's an old lady upstairs who has never had an orgasm. You have to give her one."

  The man thinks for a moment and then confidently says, "I'll do it."

  He grabs the bottle of tequila and downs it in one gulp, without flinching. He then heads to the back and after a few minutes of struggling, emerges with the pitbull's tooth in hand.

  The bar erupts in cheers and the bartender leads the man upstairs to the old lady's room. After a few minutes, the man walks out with a big smile on his face and the old lady is giggling with delight.

  The bartender hands the man the jar of money and asks, "How

  default: 175.74ms
*/

```

## Caching with Momento[‚Äã](#caching-with-momento "Direct link to Caching with Momento")

LangChain also provides a Momento-based cache. [Momento](https://gomomento.com) is a distributed, serverless cache that requires zero setup or infrastructure maintenance. Given Momento's compatibility with Node.js, browser, and edge environments, ensure you install the relevant package.

To install for **Node.js**:

* npm
* Yarn
* pnpm

```
npm install @gomomento/sdk

```

```
yarn add @gomomento/sdk

```

```
pnpm add @gomomento/sdk

```

To install for **browser/edge workers**:

* npm
* Yarn
* pnpm

```
npm install @gomomento/sdk-web

```

```
yarn add @gomomento/sdk-web

```

```
pnpm add @gomomento/sdk-web

```

Next you'll need to sign up and create an API key. Once you've done that, pass a `cache` option when you instantiate the LLM like this:

```
import { OpenAI } from "@langchain/openai";
import {
  CacheClient,
  Configurations,
  CredentialProvider,
} from "@gomomento/sdk";
import { MomentoCache } from "@langchain/community/caches/momento";

// See https://github.com/momentohq/client-sdk-javascript for connection options
const client = new CacheClient({
  configuration: Configurations.Laptop.v1(),
  credentialProvider: CredentialProvider.fromEnvironmentVariable({
    environmentVariableName: "MOMENTO_API_KEY",
  }),
  defaultTtlSeconds: 60 * 60 * 24,
});
const cache = await MomentoCache.fromProps({
  client,
  cacheName: "langchain",
});

const model = new OpenAI({ cache });

```

#### API Reference:

* OpenAI from `@langchain/openai`
* MomentoCache from `@langchain/community/caches/momento`

## Caching with Redis[‚Äã](#caching-with-redis "Direct link to Caching with Redis")

LangChain also provides a Redis-based cache. This is useful if you want to share the cache across multiple processes or servers. To use it, you'll need to install the `redis` package:

* npm
* Yarn
* pnpm

```
npm install ioredis

```

```
yarn add ioredis

```

```
pnpm add ioredis

```

Then, you can pass a `cache` option when you instantiate the LLM. For example:

```
import { OpenAI } from "@langchain/openai";
import { RedisCache } from "@langchain/community/caches/ioredis";
import { Redis } from "ioredis";

// See https://github.com/redis/ioredis for connection options
const client = new Redis({});

const cache = new RedisCache(client);

const model = new OpenAI({ cache });

```

## Caching with Upstash Redis[‚Äã](#caching-with-upstash-redis "Direct link to Caching with Upstash Redis")

LangChain provides an Upstash Redis-based cache. Like the Redis-based cache, this cache is useful if you want to share the cache across multiple processes or servers. The Upstash Redis client uses HTTP and supports edge environments. To use it, you'll need to install the `@upstash/redis` package:

* npm
* Yarn
* pnpm

```
npm install @upstash/redis

```

```
yarn add @upstash/redis

```

```
pnpm add @upstash/redis

```

You'll also need an [Upstash account](https://docs.upstash.com/redis#create-account) and a [Redis database](https://docs.upstash.com/redis#create-a-database) to connect to. Once you've done that, retrieve your REST URL and REST token.

Then, you can pass a `cache` option when you instantiate the LLM. For example:

```
import { OpenAI } from "@langchain/openai";
import { UpstashRedisCache } from "@langchain/community/caches/upstash_redis";

// See https://docs.upstash.com/redis/howto/connectwithupstashredis#quick-start for connection options
const cache = new UpstashRedisCache({
  config: {
    url: "UPSTASH_REDIS_REST_URL",
    token: "UPSTASH_REDIS_REST_TOKEN",
  },
  ttl: 3600,
});

const model = new OpenAI({ cache });

```

#### API Reference:

* OpenAI from `@langchain/openai`
* UpstashRedisCache from `@langchain/community/caches/upstash_redis`

You can also directly pass in a previously created [@upstash/redis](https://docs.upstash.com/redis/sdks/javascriptsdk/overview) client instance:

```
import { Redis } from "@upstash/redis";
import https from "https";

import { OpenAI } from "@langchain/openai";
import { UpstashRedisCache } from "@langchain/community/caches/upstash_redis";

// const client = new Redis({
//   url: process.env.UPSTASH_REDIS_REST_URL!,
//   token: process.env.UPSTASH_REDIS_REST_TOKEN!,
//   agent: new https.Agent({ keepAlive: true }),
// });

// Or simply call Redis.fromEnv() to automatically load the UPSTASH_REDIS_REST_URL and UPSTASH_REDIS_REST_TOKEN environment variables.
const client = Redis.fromEnv({
  agent: new https.Agent({ keepAlive: true }),
});

const cache = new UpstashRedisCache({ client });
const model = new OpenAI({ cache });

```

#### API Reference:

* OpenAI from `@langchain/openai`
* UpstashRedisCache from `@langchain/community/caches/upstash_redis`

## Caching with Vercel KV[‚Äã](#caching-with-vercel-kv "Direct link to Caching with Vercel KV")

LangChain provides an Vercel KV-based cache. Like the Redis-based cache, this cache is useful if you want to share the cache across multiple processes or servers. The Vercel KV client uses HTTP and supports edge environments. To use it, you'll need to install the `@vercel/kv` package:

* npm
* Yarn
* pnpm

```
npm install @vercel/kv

```

```
yarn add @vercel/kv

```

```
pnpm add @vercel/kv

```

You'll also need an Vercel account and a [KV database](https://vercel.com/docs/storage/vercel-kv/kv-reference) to connect to. Once you've done that, retrieve your REST URL and REST token.

Then, you can pass a `cache` option when you instantiate the LLM. For example:

```
import { OpenAI } from "@langchain/openai";
import { VercelKVCache } from "@langchain/community/caches/vercel_kv";
import { createClient } from "@vercel/kv";

// See https://vercel.com/docs/storage/vercel-kv/kv-reference#createclient-example for connection options
const cache = new VercelKVCache({
  client: createClient({
    url: "VERCEL_KV_API_URL",
    token: "VERCEL_KV_API_TOKEN",
  }),
  ttl: 3600,
});

const model = new OpenAI({ cache });

```

#### API Reference:

* OpenAI from `@langchain/openai`
* VercelKVCache from `@langchain/community/caches/vercel_kv`

## Caching with Cloudflare KV[‚Äã](#caching-with-cloudflare-kv "Direct link to Caching with Cloudflare KV")

info

This integration is only supported in Cloudflare Workers.

If you're deploying your project as a Cloudflare Worker, you can use LangChain's Cloudflare KV-powered LLM cache.

For information on how to set up KV in Cloudflare, see [the official documentation](https://developers.cloudflare.com/kv/).

**Note:** If you are using TypeScript, you may need to install types if they aren't already present:

* npm
* Yarn
* pnpm

```
npm install -S @cloudflare/workers-types

```

```
yarn add @cloudflare/workers-types

```

```
pnpm add @cloudflare/workers-types

```

```
import type { KVNamespace } from "@cloudflare/workers-types";

import { OpenAI } from "@langchain/openai";
import { CloudflareKVCache } from "@langchain/cloudflare";

export interface Env {
  KV_NAMESPACE: KVNamespace;
  OPENAI_API_KEY: string;
}

export default {
  async fetch(_request: Request, env: Env) {
    try {
      const cache = new CloudflareKVCache(env.KV_NAMESPACE);
      const model = new OpenAI({
        cache,
        model: "gpt-3.5-turbo-instruct",
        apiKey: env.OPENAI_API_KEY,
      });
      const response = await model.invoke("How are you today?");
      return new Response(JSON.stringify(response), {
        headers: { "content-type": "application/json" },
      });
    } catch (err: any) {
      console.log(err.message);
      return new Response(err.message, { status: 500 });
    }
  },
};

```

#### API Reference:

* OpenAI from `@langchain/openai`
* CloudflareKVCache from `@langchain/cloudflare`

## Caching on the File System[‚Äã](#caching-on-the-file-system "Direct link to Caching on the File System")

danger

This cache is not recommended for production use. It is only intended for local development.

LangChain provides a simple file system cache. By default the cache is stored a temporary directory, but you can specify a custom directory if you want.

```
const cache = await LocalFileCache.create();

```

## Next steps[‚Äã](#next-steps "Direct link to Next steps")

You've now learned how to cache model responses to save time and money.

Next, check out the other how-to guides on LLMs, like [how to create your own custom LLM class](/docs/how%5Fto/custom%5Fllm).

---

#### Was this page helpful?

  
#### You can also leave detailed feedback [on GitHub](https://github.com/langchain-ai/langchainjs/issues/new?assignees=&labels=03+-+Documentation&projects=&template=documentation.yml&title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).

[PreviousHow to use few shot examples in chat models](/docs/how%5Fto/few%5Fshot%5Fexamples%5Fchat)[NextHow to cache chat model responses](/docs/how%5Fto/chat%5Fmodel%5Fcaching)

* [In Memory Cache](#in-memory-cache)
* [Caching with Momento](#caching-with-momento)
* [Caching with Redis](#caching-with-redis)
* [Caching with Upstash Redis](#caching-with-upstash-redis)
* [Caching with Vercel KV](#caching-with-vercel-kv)
* [Caching with Cloudflare KV](#caching-with-cloudflare-kv)
* [Caching on the File System](#caching-on-the-file-system)
* [Next steps](#next-steps)

Community

* [Twitter](https://twitter.com/LangChainAI)

GitHub

* [Python](https://github.com/langchain-ai/langchain)
* [JS/TS](https://github.com/langchain-ai/langchainjs)

More

* [Homepage](https://langchain.com)
* [Blog](https://blog.langchain.dev)

Copyright ¬© 2025 LangChain, Inc.
[Skip to main content](#%5F%5Fdocusaurus%5FskipToContent%5Ffallback)

**Help us build the JS tools that power AI apps at companies like Replit, Uber, LinkedIn, GitLab, and more. [Join our team!](https://jobs.ashbyhq.com/langchain/05efa205-8560-43fd-bfcc-3f7697561cfb?utm%5Fsource=https%3A%2F%2Fjs.langchain.com%2F&utm%5Fcampaign=langchainjs%5Fdocs)**

[![ü¶úÔ∏èüîó Langchain](/img/brand/wordmark.png)![ü¶úÔ∏èüîó Langchain](/img/brand/wordmark-dark.png)](/)[Integrations](/docs/integrations/platforms/)[API Reference](https://v03.api.js.langchain.com)

[More](#)
* [People](/docs/people/)
* [Community](/docs/community)
* [Error reference](/docs/troubleshooting/errors)
* [External guides](/docs/additional%5Fresources/tutorials)
* [Contributing](/docs/contributing)

[v0.3](#)
* [v0.3](/docs/introduction)
* [v0.2](https://js.langchain.com/v0.2/docs/introduction)
* [v0.1](https://js.langchain.com/v0.1/docs/get%5Fstarted/introduction)

[ü¶úüîó](#)
* [LangSmith](https://smith.langchain.com)
* [LangSmith Docs](https://docs.smith.langchain.com)
* [LangChain Hub](https://smith.langchain.com/hub)
* [LangServe](https://github.com/langchain-ai/langserve)
* [Python Docs](https://python.langchain.com/)

[Chat](https://chatjs.langchain.com)[](https://github.com/langchain-ai/langchainjs)

Search

* [Introduction](/docs/introduction)
* [Tutorials](/docs/tutorials/)  
   * [Build a Question Answering application over a Graph Database](/docs/tutorials/graph)  
   * [Tutorials](/docs/tutorials/)  
   * [Build a simple LLM application with chat models and prompt templates](/docs/tutorials/llm%5Fchain)  
   * [Build a Chatbot](/docs/tutorials/chatbot)  
   * [Build a Retrieval Augmented Generation (RAG) App: Part 2](/docs/tutorials/qa%5Fchat%5Fhistory)  
   * [Build an Extraction Chain](/docs/tutorials/extraction)  
   * [Tagging](/docs/tutorials/classification)  
   * [Build a Retrieval Augmented Generation (RAG) App: Part 1](/docs/tutorials/rag)  
   * [Build a semantic search engine](/docs/tutorials/retrievers)  
   * [Build a Question/Answering system over SQL data](/docs/tutorials/sql%5Fqa)  
   * [Summarize Text](/docs/tutorials/summarization)
* [How-to guides](/docs/how%5Fto/)  
   * [How-to guides](/docs/how%5Fto/)  
   * [How to add memory to chatbots](/docs/how%5Fto/chatbots%5Fmemory)  
   * [How to use example selectors](/docs/how%5Fto/example%5Fselectors)  
   * [Installation](/docs/how%5Fto/installation)  
   * [How to stream responses from an LLM](/docs/how%5Fto/streaming%5Fllm)  
   * [How to stream chat model responses](/docs/how%5Fto/chat%5Fstreaming)  
   * [How to embed text data](/docs/how%5Fto/embed%5Ftext)  
   * [How to use few shot examples in chat models](/docs/how%5Fto/few%5Fshot%5Fexamples%5Fchat)  
   * [How to cache model responses](/docs/how%5Fto/llm%5Fcaching)  
   * [How to cache chat model responses](/docs/how%5Fto/chat%5Fmodel%5Fcaching)  
   * [Richer outputs](/docs/how%5Fto/custom%5Fllm)  
   * [How to use few shot examples](/docs/how%5Fto/few%5Fshot%5Fexamples)  
   * [How to use output parsers to parse an LLM response into structured format](/docs/how%5Fto/output%5Fparser%5Fstructured)  
   * [How to return structured data from a model](/docs/how%5Fto/structured%5Foutput)  
   * [How to add ad-hoc tool calling capability to LLMs and Chat Models](/docs/how%5Fto/tools%5Fprompting)  
   * [Richer outputs](/docs/how%5Fto/custom%5Fchat)  
   * [How to do per-user retrieval](/docs/how%5Fto/qa%5Fper%5Fuser)  
   * [How to track token usage](/docs/how%5Fto/chat%5Ftoken%5Fusage%5Ftracking)  
   * [How to track token usage](/docs/how%5Fto/llm%5Ftoken%5Fusage%5Ftracking)  
   * [How to pass through arguments from one step to the next](/docs/how%5Fto/passthrough)  
   * [How to compose prompts together](/docs/how%5Fto/prompts%5Fcomposition)  
   * [How to use legacy LangChain Agents (AgentExecutor)](/docs/how%5Fto/agent%5Fexecutor)  
   * [How to add values to a chain's state](/docs/how%5Fto/assign)  
   * [How to attach runtime arguments to a Runnable](/docs/how%5Fto/binding)  
   * [How to cache embedding results](/docs/how%5Fto/caching%5Fembeddings)  
   * [How to attach callbacks to a module](/docs/how%5Fto/callbacks%5Fattach)  
   * [How to pass callbacks into a module constructor](/docs/how%5Fto/callbacks%5Fconstructor)  
   * [How to dispatch custom callback events](/docs/how%5Fto/callbacks%5Fcustom%5Fevents)  
   * [How to pass callbacks in at runtime](/docs/how%5Fto/callbacks%5Fruntime)  
   * [How to await callbacks in serverless environments](/docs/how%5Fto/callbacks%5Fserverless)  
   * [How to cancel execution](/docs/how%5Fto/cancel%5Fexecution)  
   * [How to split by character](/docs/how%5Fto/character%5Ftext%5Fsplitter)  
   * [How to init any model in one line](/docs/how%5Fto/chat%5Fmodels%5Funiversal%5Finit)  
   * [How to do retrieval](/docs/how%5Fto/chatbots%5Fretrieval)  
   * [How to add tools to chatbots](/docs/how%5Fto/chatbots%5Ftools)  
   * [How to split code](/docs/how%5Fto/code%5Fsplitter)  
   * [How to do retrieval with contextual compression](/docs/how%5Fto/contextual%5Fcompression)  
   * [How to convert Runnables to Tools](/docs/how%5Fto/convert%5Frunnable%5Fto%5Ftool)  
   * [How to create custom callback handlers](/docs/how%5Fto/custom%5Fcallbacks)  
   * [How to write a custom retriever class](/docs/how%5Fto/custom%5Fretriever)  
   * [How to create Tools](/docs/how%5Fto/custom%5Ftools)  
   * [How to debug your LLM apps](/docs/how%5Fto/debugging)  
   * [How to load CSV data](/docs/how%5Fto/document%5Floader%5Fcsv)  
   * [How to write a custom document loader](/docs/how%5Fto/document%5Floader%5Fcustom)  
   * [How to load data from a directory](/docs/how%5Fto/document%5Floader%5Fdirectory)  
   * [How to load HTML](/docs/how%5Fto/document%5Floader%5Fhtml)  
   * [How to load Markdown](/docs/how%5Fto/document%5Floader%5Fmarkdown)  
   * [How to load PDF files](/docs/how%5Fto/document%5Floader%5Fpdf)  
   * [How to load JSON data](/docs/how%5Fto/document%5Floaders%5Fjson)  
   * [How to combine results from multiple retrievers](/docs/how%5Fto/ensemble%5Fretriever)  
   * [How to select examples from a LangSmith dataset](/docs/how%5Fto/example%5Fselectors%5Flangsmith)  
   * [How to select examples by length](/docs/how%5Fto/example%5Fselectors%5Flength%5Fbased)  
   * [How to select examples by similarity](/docs/how%5Fto/example%5Fselectors%5Fsimilarity)  
   * [How to use reference examples](/docs/how%5Fto/extraction%5Fexamples)  
   * [How to handle long text](/docs/how%5Fto/extraction%5Flong%5Ftext)  
   * [How to do extraction without using function calling](/docs/how%5Fto/extraction%5Fparse)  
   * [Fallbacks](/docs/how%5Fto/fallbacks)  
   * [Few Shot Prompt Templates](/docs/how%5Fto/few%5Fshot)  
   * [How to filter messages](/docs/how%5Fto/filter%5Fmessages)  
   * [How to run custom functions](/docs/how%5Fto/functions)  
   * [How to build an LLM generated UI](/docs/how%5Fto/generative%5Fui)  
   * [How to construct knowledge graphs](/docs/how%5Fto/graph%5Fconstructing)  
   * [How to map values to a database](/docs/how%5Fto/graph%5Fmapping)  
   * [How to improve results with prompting](/docs/how%5Fto/graph%5Fprompting)  
   * [How to add a semantic layer over the database](/docs/how%5Fto/graph%5Fsemantic)  
   * [How to reindex data to keep your vectorstore in-sync with the underlying data source](/docs/how%5Fto/indexing)  
   * [LangChain Expression Language Cheatsheet](/docs/how%5Fto/lcel%5Fcheatsheet)  
   * [How to get log probabilities](/docs/how%5Fto/logprobs)  
   * [How to merge consecutive messages of the same type](/docs/how%5Fto/merge%5Fmessage%5Fruns)  
   * [How to add message history](/docs/how%5Fto/message%5Fhistory)  
   * [How to migrate from legacy LangChain agents to LangGraph](/docs/how%5Fto/migrate%5Fagent)  
   * [How to generate multiple embeddings per document](/docs/how%5Fto/multi%5Fvector)  
   * [How to pass multimodal data directly to models](/docs/how%5Fto/multimodal%5Finputs)  
   * [How to use multimodal prompts](/docs/how%5Fto/multimodal%5Fprompts)  
   * [How to generate multiple queries to retrieve data for](/docs/how%5Fto/multiple%5Fqueries)  
   * [How to try to fix errors in output parsing](/docs/how%5Fto/output%5Fparser%5Ffixing)  
   * [How to parse JSON output](/docs/how%5Fto/output%5Fparser%5Fjson)  
   * [How to parse XML output](/docs/how%5Fto/output%5Fparser%5Fxml)  
   * [How to invoke runnables in parallel](/docs/how%5Fto/parallel)  
   * [How to retrieve the whole document for a chunk](/docs/how%5Fto/parent%5Fdocument%5Fretriever)  
   * [How to partially format prompt templates](/docs/how%5Fto/prompts%5Fpartial)  
   * [How to add chat history](/docs/how%5Fto/qa%5Fchat%5Fhistory%5Fhow%5Fto)  
   * [How to return citations](/docs/how%5Fto/qa%5Fcitations)  
   * [How to return sources](/docs/how%5Fto/qa%5Fsources)  
   * [How to stream from a question-answering chain](/docs/how%5Fto/qa%5Fstreaming)  
   * [How to construct filters](/docs/how%5Fto/query%5Fconstructing%5Ffilters)  
   * [How to add examples to the prompt](/docs/how%5Fto/query%5Ffew%5Fshot)  
   * [How to deal with high cardinality categorical variables](/docs/how%5Fto/query%5Fhigh%5Fcardinality)  
   * [How to handle multiple queries](/docs/how%5Fto/query%5Fmultiple%5Fqueries)  
   * [How to handle multiple retrievers](/docs/how%5Fto/query%5Fmultiple%5Fretrievers)  
   * [How to handle cases where no queries are generated](/docs/how%5Fto/query%5Fno%5Fqueries)  
   * [How to recursively split text by characters](/docs/how%5Fto/recursive%5Ftext%5Fsplitter)  
   * [How to reduce retrieval latency](/docs/how%5Fto/reduce%5Fretrieval%5Flatency)  
   * [How to route execution within a chain](/docs/how%5Fto/routing)  
   * [How to do "self-querying" retrieval](/docs/how%5Fto/self%5Fquery)  
   * [How to chain runnables](/docs/how%5Fto/sequence)  
   * [How to split text by tokens](/docs/how%5Fto/split%5Fby%5Ftoken)  
   * [How to deal with large databases](/docs/how%5Fto/sql%5Flarge%5Fdb)  
   * [How to use prompting to improve results](/docs/how%5Fto/sql%5Fprompting)  
   * [How to do query validation](/docs/how%5Fto/sql%5Fquery%5Fchecking)  
   * [How to stream agent data to the client](/docs/how%5Fto/stream%5Fagent%5Fclient)  
   * [How to stream structured output to the client](/docs/how%5Fto/stream%5Ftool%5Fclient)  
   * [How to stream](/docs/how%5Fto/streaming)  
   * [How to create a time-weighted retriever](/docs/how%5Fto/time%5Fweighted%5Fvectorstore)  
   * [How to return artifacts from a tool](/docs/how%5Fto/tool%5Fartifacts)  
   * [How to use chat models to call tools](/docs/how%5Fto/tool%5Fcalling)  
   * [How to disable parallel tool calling](/docs/how%5Fto/tool%5Fcalling%5Fparallel)  
   * [How to call tools with multimodal data](/docs/how%5Fto/tool%5Fcalls%5Fmultimodal)  
   * [How to force tool calling behavior](/docs/how%5Fto/tool%5Fchoice)  
   * [How to access the RunnableConfig from a tool](/docs/how%5Fto/tool%5Fconfigure)  
   * [How to pass tool outputs to chat models](/docs/how%5Fto/tool%5Fresults%5Fpass%5Fto%5Fmodel)  
   * [How to pass run time values to tools](/docs/how%5Fto/tool%5Fruntime)  
   * [How to stream events from a tool](/docs/how%5Fto/tool%5Fstream%5Fevents)  
   * [How to stream tool calls](/docs/how%5Fto/tool%5Fstreaming)  
   * [How to use LangChain tools](/docs/how%5Fto/tools%5Fbuiltin)  
   * [How to handle tool errors](/docs/how%5Fto/tools%5Ferror)  
   * [How to use few-shot prompting with tool calling](/docs/how%5Fto/tools%5Ffew%5Fshot)  
   * [How to trim messages](/docs/how%5Fto/trim%5Fmessages)  
   * [How use a vector store to retrieve data](/docs/how%5Fto/vectorstore%5Fretriever)  
   * [How to create and query vector stores](/docs/how%5Fto/vectorstores)
* [Conceptual Guide](/docs/concepts/)  
   * [Agents](/docs/concepts/agents)  
   * [Architecture](/docs/concepts/architecture)  
   * [Callbacks](/docs/concepts/callbacks)  
   * [Chat history](/docs/concepts/chat%5Fhistory)  
   * [Chat models](/docs/concepts/chat%5Fmodels)  
   * [Document loaders](/docs/concepts/document%5Floaders)  
   * [Embedding models](/docs/concepts/embedding%5Fmodels)  
   * [Evaluation](/docs/concepts/evaluation)  
   * [Example selectors](/docs/concepts/example%5Fselectors)  
   * [Few-shot prompting](/docs/concepts/few%5Fshot%5Fprompting)  
   * [Conceptual guide](/docs/concepts/)  
   * [Key-value stores](/docs/concepts/key%5Fvalue%5Fstores)  
   * [LangChain Expression Language (LCEL)](/docs/concepts/lcel)  
   * [Messages](/docs/concepts/messages)  
   * [Multimodality](/docs/concepts/multimodality)  
   * [Output parsers](/docs/concepts/output%5Fparsers)  
   * [Prompt Templates](/docs/concepts/prompt%5Ftemplates)  
   * [Retrieval augmented generation (rag)](/docs/concepts/rag)  
   * [Retrieval](/docs/concepts/retrieval)  
   * [Retrievers](/docs/concepts/retrievers)  
   * [Runnable interface](/docs/concepts/runnables)  
   * [Streaming](/docs/concepts/streaming)  
   * [Structured outputs](/docs/concepts/structured%5Foutputs)  
   * [t](/docs/concepts/t)  
   * [String-in, string-out llms](/docs/concepts/text%5Fllms)  
   * [Text splitters](/docs/concepts/text%5Fsplitters)  
   * [Tokens](/docs/concepts/tokens)  
   * [Tool calling](/docs/concepts/tool%5Fcalling)  
   * [Tools](/docs/concepts/tools)  
   * [Tracing](/docs/concepts/tracing)  
   * [Vector stores](/docs/concepts/vectorstores)  
   * [Why LangChain?](/docs/concepts/why%5Flangchain)
* Ecosystem  
   * [ü¶úüõ†Ô∏è LangSmith](https://docs.smith.langchain.com/)  
   * [ü¶úüï∏Ô∏è LangGraph.js](https://langchain-ai.github.io/langgraphjs/)
* Versions  
   * [v0.3](/docs/versions/v0%5F3/)  
   * [v0.2](/docs/versions/v0%5F2/)  
   * [Migrating from v0.0 memory](/docs/versions/migrating%5Fmemory/)  
         * [How to migrate to LangGraph memory](/docs/versions/migrating%5Fmemory/)  
         * [How to use BaseChatMessageHistory with LangGraph](/docs/versions/migrating%5Fmemory/chat%5Fhistory)  
         * [Migrating off ConversationTokenBufferMemory](/docs/versions/migrating%5Fmemory/conversation%5Fbuffer%5Fwindow%5Fmemory)  
         * [Migrating off ConversationSummaryMemory or ConversationSummaryBufferMemory](/docs/versions/migrating%5Fmemory/conversation%5Fsummary%5Fmemory)  
   * [Release Policy](/docs/versions/release%5Fpolicy)
* [Security](/docs/security)

* [How-to guides](/docs/how%5Fto/)
* How to stream

On this page

# How to stream

Prerequisites

This guide assumes familiarity with the following concepts:

* [Chat models](/docs/concepts/chat%5Fmodels)
* [LangChain Expression Language](/docs/concepts/lcel)
* [Output parsers](/docs/concepts/output%5Fparsers)

Streaming is critical in making applications based on LLMs feel responsive to end-users.

Important LangChain primitives like LLMs, parsers, prompts, retrievers, and agents implement the LangChain Runnable Interface.

This interface provides two general approaches to stream content:

* `.stream()`: a default implementation of streaming that streams the final output from the chain.
* `streamEvents()` and `streamLog()`: these provide a way to stream both intermediate steps and final output from the chain.

Let‚Äôs take a look at both approaches!

For a higher-level overview of streaming techniques in LangChain, see[this section of the conceptual guide](/docs/concepts/streaming).

# Using Stream

All `Runnable` objects implement a method called stream.

These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.

Streaming is only possible if all steps in the program know how to process an **input stream**; i.e., process an input chunk one at a time, and yield a corresponding output chunk.

The complexity of this processing can vary, from straightforward tasks like emitting tokens produced by an LLM, to more challenging ones like streaming parts of JSON results before the entire JSON is complete.

The best place to start exploring streaming is with the single most important components in LLM apps ‚Äì the models themselves!

## LLMs and Chat Models[‚Äã](#llms-and-chat-models "Direct link to LLMs and Chat Models")

Large language models can take several seconds to generate a complete response to a query. This is far slower than the **\~200-300 ms**threshold at which an application feels responsive to an end user.

The key strategy to make the application feel more responsive is to show intermediate progress; e.g., to stream the output from the model token by token.

```
import "dotenv/config";

```

### Pick your chat model:

* Groq
* OpenAI
* Anthropic
* Google Gemini
* FireworksAI
* MistralAI
* VertexAI

#### Install dependencies

tip

See [this section for general instructions on installing integration packages](/docs/how%5Fto/installation/#installing-integration-packages).

* npm
* yarn
* pnpm

```
npm i @langchain/groq 

```

```
yarn add @langchain/groq 

```

```
pnpm add @langchain/groq 

```

#### Add environment variables

```
GROQ_API_KEY=your-api-key

```

#### Instantiate the model

```
import { ChatGroq } from "@langchain/groq";

const model = new ChatGroq({
  model: "llama-3.3-70b-versatile",
  temperature: 0
});

```

#### Install dependencies

tip

See [this section for general instructions on installing integration packages](/docs/how%5Fto/installation/#installing-integration-packages).

* npm
* yarn
* pnpm

```
npm i @langchain/openai 

```

```
yarn add @langchain/openai 

```

```
pnpm add @langchain/openai 

```

#### Add environment variables

```
OPENAI_API_KEY=your-api-key

```

#### Instantiate the model

```
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "gpt-4o-mini",
  temperature: 0
});

```

#### Install dependencies

tip

See [this section for general instructions on installing integration packages](/docs/how%5Fto/installation/#installing-integration-packages).

* npm
* yarn
* pnpm

```
npm i @langchain/anthropic 

```

```
yarn add @langchain/anthropic 

```

```
pnpm add @langchain/anthropic 

```

#### Add environment variables

```
ANTHROPIC_API_KEY=your-api-key

```

#### Instantiate the model

```
import { ChatAnthropic } from "@langchain/anthropic";

const model = new ChatAnthropic({
  model: "claude-3-5-sonnet-20240620",
  temperature: 0
});

```

#### Install dependencies

tip

See [this section for general instructions on installing integration packages](/docs/how%5Fto/installation/#installing-integration-packages).

* npm
* yarn
* pnpm

```
npm i @langchain/google-genai 

```

```
yarn add @langchain/google-genai 

```

```
pnpm add @langchain/google-genai 

```

#### Add environment variables

```
GOOGLE_API_KEY=your-api-key

```

#### Instantiate the model

```
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";

const model = new ChatGoogleGenerativeAI({
  model: "gemini-2.0-flash",
  temperature: 0
});

```

#### Install dependencies

tip

See [this section for general instructions on installing integration packages](/docs/how%5Fto/installation/#installing-integration-packages).

* npm
* yarn
* pnpm

```
npm i @langchain/community 

```

```
yarn add @langchain/community 

```

```
pnpm add @langchain/community 

```

#### Add environment variables

```
FIREWORKS_API_KEY=your-api-key

```

#### Instantiate the model

```
import { ChatFireworks } from "@langchain/community/chat_models/fireworks";

const model = new ChatFireworks({
  model: "accounts/fireworks/models/llama-v3p1-70b-instruct",
  temperature: 0
});

```

#### Install dependencies

tip

See [this section for general instructions on installing integration packages](/docs/how%5Fto/installation/#installing-integration-packages).

* npm
* yarn
* pnpm

```
npm i @langchain/mistralai 

```

```
yarn add @langchain/mistralai 

```

```
pnpm add @langchain/mistralai 

```

#### Add environment variables

```
MISTRAL_API_KEY=your-api-key

```

#### Instantiate the model

```
import { ChatMistralAI } from "@langchain/mistralai";

const model = new ChatMistralAI({
  model: "mistral-large-latest",
  temperature: 0
});

```

#### Install dependencies

tip

See [this section for general instructions on installing integration packages](/docs/how%5Fto/installation/#installing-integration-packages).

* npm
* yarn
* pnpm

```
npm i @langchain/google-vertexai 

```

```
yarn add @langchain/google-vertexai 

```

```
pnpm add @langchain/google-vertexai 

```

#### Add environment variables

```
GOOGLE_APPLICATION_CREDENTIALS=credentials.json

```

#### Instantiate the model

```
import { ChatVertexAI } from "@langchain/google-vertexai";

const model = new ChatVertexAI({
  model: "gemini-1.5-flash",
  temperature: 0
});

```

```
const stream = await model.stream("Hello! Tell me about yourself.");
const chunks = [];
for await (const chunk of stream) {
  chunks.push(chunk);
  console.log(`${chunk.content}|`);
}

```

```
|
Hello|
!|
 I'm|
 a|
 large|
 language|
 model|
 developed|
 by|
 Open|
AI|
 called|
 GPT|
-|
4|
,|
 based|
 on|
 the|
 Gener|
ative|
 Pre|
-trained|
 Transformer|
 architecture|
.|
 I'm|
 designed|
 to|
 understand|
 and|
 generate|
 human|
-like|
 text|
 based|
 on|
 the|
 input|
 I|
 receive|
.|
 My|
 primary|
 function|
 is|
 to|
 assist|
 with|
 answering|
 questions|
,|
 providing|
 information|
,|
 and|
 engaging|
 in|
 various|
 types|
 of|
 conversations|
.|
 While|
 I|
 don't|
 have|
 personal|
 experiences|
 or|
 emotions|
,|
 I'm|
 trained|
 on|
 diverse|
 datasets|
 that|
 enable|
 me|
 to|
 provide|
 useful|
 and|
 relevant|
 information|
 across|
 a|
 wide|
 array|
 of|
 topics|
.|
 How|
 can|
 I|
 assist|
 you|
 today|
?|
|
|

```

Let‚Äôs have a look at one of the raw chunks:

```
chunks[0];

```

```
AIMessageChunk {
  lc_serializable: true,
  lc_kwargs: {
    content: '',
    tool_call_chunks: [],
    additional_kwargs: {},
    id: 'chatcmpl-9lO8YUEcX7rqaxxevelHBtl1GaWoo',
    tool_calls: [],
    invalid_tool_calls: [],
    response_metadata: {}
  },
  lc_namespace: [ 'langchain_core', 'messages' ],
  content: '',
  name: undefined,
  additional_kwargs: {},
  response_metadata: { prompt: 0, completion: 0, finish_reason: null },
  id: 'chatcmpl-9lO8YUEcX7rqaxxevelHBtl1GaWoo',
  tool_calls: [],
  invalid_tool_calls: [],
  tool_call_chunks: [],
  usage_metadata: undefined
}

```

We got back something called an `AIMessageChunk`. This chunk represents a part of an `AIMessage`.

Message chunks are additive by design ‚Äì one can simply add them up using the `.concat()` method to get the state of the response so far!

```
let finalChunk = chunks[0];

for (const chunk of chunks.slice(1, 5)) {
  finalChunk = finalChunk.concat(chunk);
}

finalChunk;

```

```
AIMessageChunk {
  lc_serializable: true,
  lc_kwargs: {
    content: "Hello! I'm a",
    additional_kwargs: {},
    response_metadata: { prompt: 0, completion: 0, finish_reason: null },
    tool_call_chunks: [],
    id: 'chatcmpl-9lO8YUEcX7rqaxxevelHBtl1GaWoo',
    tool_calls: [],
    invalid_tool_calls: []
  },
  lc_namespace: [ 'langchain_core', 'messages' ],
  content: "Hello! I'm a",
  name: undefined,
  additional_kwargs: {},
  response_metadata: { prompt: 0, completion: 0, finish_reason: null },
  id: 'chatcmpl-9lO8YUEcX7rqaxxevelHBtl1GaWoo',
  tool_calls: [],
  invalid_tool_calls: [],
  tool_call_chunks: [],
  usage_metadata: undefined
}

```

## Chains[‚Äã](#chains "Direct link to Chains")

Virtually all LLM applications involve more steps than just a call to a language model.

Let‚Äôs build a simple chain using `LangChain Expression Language`(`LCEL`) that combines a prompt, model and a parser and verify that streaming works.

We will use `StringOutputParser` to parse the output from the model. This is a simple parser that extracts the content field from an`AIMessageChunk`, giving us the `token` returned by the model.

tip

LCEL is a declarative way to specify a ‚Äúprogram‚Äù by chainining together different LangChain primitives. Chains created using LCEL benefit from an automatic implementation of stream, allowing streaming of the final output. In fact, chains created with LCEL implement the entire standard Runnable interface.

```
import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const prompt = ChatPromptTemplate.fromTemplate("Tell me a joke about {topic}");

const parser = new StringOutputParser();

const chain = prompt.pipe(model).pipe(parser);

const stream = await chain.stream({
  topic: "parrot",
});

for await (const chunk of stream) {
  console.log(`${chunk}|`);
}

```

```
|
Sure|
,|
 here's|
 a|
 joke|
 for|
 you|
:

|
Why|
 did|
 the|
 par|
rot|
 sit|
 on|
 the|
 stick|
?

|
Because|
 it|
 wanted|
 to|
 be|
 a|
 "|
pol|
ly|
-stick|
-al|
"|
 observer|
!|
|
|

```

note

You do not have to use the `LangChain Expression Language` to use LangChain and can instead rely on a standard **imperative** programming approach by caling `invoke`, `batch` or `stream` on each component individually, assigning the results to variables and then using them downstream as you see fit.

If that works for your needs, then that‚Äôs fine by us üëå!

### Working with Input Streams[‚Äã](#working-with-input-streams "Direct link to Working with Input Streams")

What if you wanted to stream JSON from the output as it was being generated?

If you were to rely on `JSON.parse` to parse the partial json, the parsing would fail as the partial json wouldn‚Äôt be valid json.

You‚Äôd likely be at a complete loss of what to do and claim that it wasn‚Äôt possible to stream JSON.

Well, turns out there is a way to do it - the parser needs to operate on the **input stream**, and attempt to ‚Äúauto-complete‚Äù the partial json into a valid state.

Let‚Äôs see such a parser in action to understand what this means.

```
import { JsonOutputParser } from "@langchain/core/output_parsers";

const chain = model.pipe(new JsonOutputParser());
const stream = await chain.stream(
  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`
);

for await (const chunk of stream) {
  console.log(chunk);
}

```

```
{
  countries: [
    { name: 'France', population: 67390000 },
    { name: 'Spain', population: 47350000 },
    { name: 'Japan', population: 125800000 }
  ]
}

```

Now, let‚Äôs **break** streaming. We‚Äôll use the previous example and append an extraction function at the end that extracts the country names from the finalized JSON. Since this new last step is just a function call with no defined streaming behavior, the streaming output from previous steps is aggregated, then passed as a single input to the function.

danger

Any steps in the chain that operate on **finalized inputs** rather than on **input streams** can break streaming functionality via `stream`.

tip

Later, we will discuss the `streamEvents` API which streams results from intermediate steps. This API will stream results from intermediate steps even if the chain contains steps that only operate on **finalized inputs**.

```
// A function that operates on finalized inputs
// rather than on an input_stream

// A function that does not operates on input streams and breaks streaming.
const extractCountryNames = (inputs: Record<string, any>) => {
  if (!Array.isArray(inputs.countries)) {
    return "";
  }
  return JSON.stringify(inputs.countries.map((country) => country.name));
};

const chain = model.pipe(new JsonOutputParser()).pipe(extractCountryNames);

const stream = await chain.stream(
  `output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`
);

for await (const chunk of stream) {
  console.log(chunk);
}

```

```
["France","Spain","Japan"]

```

### Non-streaming components[‚Äã](#non-streaming-components "Direct link to Non-streaming components")

Like the above example, some built-in components like Retrievers do not offer any streaming. What happens if we try to `stream` them?

```
import { OpenAIEmbeddings } from "@langchain/openai";
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { ChatPromptTemplate } from "@langchain/core/prompts";

const template = `Answer the question based only on the following context:
{context}

Question: {question}
`;
const prompt = ChatPromptTemplate.fromTemplate(template);

const vectorstore = await MemoryVectorStore.fromTexts(
  ["mitochondria is the powerhouse of the cell", "buildings are made of brick"],
  [{}, {}],
  new OpenAIEmbeddings()
);

const retriever = vectorstore.asRetriever();

const chunks = [];

for await (const chunk of await retriever.stream(
  "What is the powerhouse of the cell?"
)) {
  chunks.push(chunk);
}

console.log(chunks);

```

```
[
  [
    Document {
      pageContent: 'mitochondria is the powerhouse of the cell',
      metadata: {},
      id: undefined
    },
    Document {
      pageContent: 'buildings are made of brick',
      metadata: {},
      id: undefined
    }
  ]
]

```

Stream just yielded the final result from that component.

This is OK! Not all components have to implement streaming ‚Äì in some cases streaming is either unnecessary, difficult or just doesn‚Äôt make sense.

tip

An LCEL chain constructed using some non-streaming components will still be able to stream in a lot of cases, with streaming of partial output starting after the last non-streaming step in the chain.

Here‚Äôs an example of this:

```
import {
  RunnablePassthrough,
  RunnableSequence,
} from "@langchain/core/runnables";
import type { Document } from "@langchain/core/documents";
import { StringOutputParser } from "@langchain/core/output_parsers";

const formatDocs = (docs: Document[]) => {
  return docs.map((doc) => doc.pageContent).join("\n-----\n");
};

const retrievalChain = RunnableSequence.from([
  {
    context: retriever.pipe(formatDocs),
    question: new RunnablePassthrough(),
  },
  prompt,
  model,
  new StringOutputParser(),
]);

const stream = await retrievalChain.stream(
  "What is the powerhouse of the cell?"
);

for await (const chunk of stream) {
  console.log(`${chunk}|`);
}

```

```
|
M|
ito|
ch|
ond|
ria|
 is|
 the|
 powerhouse|
 of|
 the|
 cell|
.|
|
|

```

Now that we‚Äôve seen how the `stream` method works, let‚Äôs venture into the world of streaming events!

## Using Stream Events[‚Äã](#using-stream-events "Direct link to Using Stream Events")

Event Streaming is a **beta** API. This API may change a bit based on feedback.

note

Introduced in @langchain/core **0.1.27**.

For the `streamEvents` method to work properly:

* Any custom functions / runnables must propragate callbacks
* Set proper parameters on models to force the LLM to stream tokens.
* Let us know if anything doesn‚Äôt work as expected!

### Event Reference[‚Äã](#event-reference "Direct link to Event Reference")

Below is a reference table that shows some events that might be emitted by the various Runnable objects.

note

When streaming is implemented properly, the inputs to a runnable will not be known until after the input stream has been entirely consumed. This means that `inputs` will often be included only for `end` events and rather than for `start` events.

| event                | name               | chunk                                      | input                                           | output |
| -------------------- | ------------------ | ------------------------------------------ | ----------------------------------------------- | ------ |
| on\_llm\_start       | \[model name\]     | {‚Äòinput‚Äô: ‚Äòhello‚Äô}                         |                                                 |        |
| on\_llm\_stream      | \[model name\]     | ‚ÄòHello‚Äô or AIMessageChunk(content=‚Äúhello‚Äù) |                                                 |        |
| on\_llm\_end         | \[model name\]     | ‚ÄòHello human!‚Äô                             | {‚Äúgenerations‚Äù: \[‚Ä¶\], ‚ÄúllmOutput‚Äù: None, ‚Ä¶}    |        |
| on\_chain\_start     | format\_docs       |                                            |                                                 |        |
| on\_chain\_stream    | format\_docs       | ‚Äúhello world!, goodbye world!‚Äù             |                                                 |        |
| on\_chain\_end       | format\_docs       | \[Document(‚Ä¶)\]                            | ‚Äúhello world!, goodbye world!‚Äù                  |        |
| on\_tool\_start      | some\_tool         | {‚Äúx‚Äù: 1, ‚Äúy‚Äù: ‚Äú2‚Äù}                         |                                                 |        |
| on\_tool\_stream     | some\_tool         | {‚Äúx‚Äù: 1, ‚Äúy‚Äù: ‚Äú2‚Äù}                         |                                                 |        |
| on\_tool\_end        | some\_tool         | {‚Äúx‚Äù: 1, ‚Äúy‚Äù: ‚Äú2‚Äù}                         |                                                 |        |
| on\_retriever\_start | \[retriever name\] | {‚Äúquery‚Äù: ‚Äúhello‚Äù}                         |                                                 |        |
| on\_retriever\_chunk | \[retriever name\] | {documents: \[‚Ä¶\]}                         |                                                 |        |
| on\_retriever\_end   | \[retriever name\] | {‚Äúquery‚Äù: ‚Äúhello‚Äù}                         | {documents: \[‚Ä¶\]}                              |        |
| on\_prompt\_start    | \[template\_name\] | {‚Äúquestion‚Äù: ‚Äúhello‚Äù}                      |                                                 |        |
| on\_prompt\_end      | \[template\_name\] | {‚Äúquestion‚Äù: ‚Äúhello‚Äù}                      | ChatPromptValue(messages: \[SystemMessage, ‚Ä¶\]) |        |

`streamEvents` will also emit dispatched custom events in `v2`. Please see [this guide](/docs/how%5Fto/callbacks%5Fcustom%5Fevents/) for more.

### Chat Model[‚Äã](#chat-model "Direct link to Chat Model")

Let‚Äôs start off by looking at the events produced by a chat model.

```
const events = [];

const eventStream = await model.streamEvents("hello", { version: "v2" });

for await (const event of eventStream) {
  events.push(event);
}

console.log(events.length);

```

```
25

```

note

Hey what‚Äôs that funny version=‚Äúv2‚Äù parameter in the API?! üòæ

This is a **beta API**, and we‚Äôre almost certainly going to make some changes to it.

This version parameter will allow us to minimize such breaking changes to your code.

In short, we are annoying you now, so we don‚Äôt have to annoy you later.

Let‚Äôs take a look at the few of the start event and a few of the end events.

```
events.slice(0, 3);

```

```
[
  {
    event: 'on_chat_model_start',
    data: { input: 'hello' },
    name: 'ChatOpenAI',
    tags: [],
    run_id: 'c983e634-9f1d-4916-97d8-63c3a86102c2',
    metadata: {
      ls_provider: 'openai',
      ls_model_name: 'gpt-4o',
      ls_model_type: 'chat',
      ls_temperature: 1,
      ls_max_tokens: undefined,
      ls_stop: undefined
    }
  },
  {
    event: 'on_chat_model_stream',
    data: { chunk: [AIMessageChunk] },
    run_id: 'c983e634-9f1d-4916-97d8-63c3a86102c2',
    name: 'ChatOpenAI',
    tags: [],
    metadata: {
      ls_provider: 'openai',
      ls_model_name: 'gpt-4o',
      ls_model_type: 'chat',
      ls_temperature: 1,
      ls_max_tokens: undefined,
      ls_stop: undefined
    }
  },
  {
    event: 'on_chat_model_stream',
    run_id: 'c983e634-9f1d-4916-97d8-63c3a86102c2',
    name: 'ChatOpenAI',
    tags: [],
    metadata: {
      ls_provider: 'openai',
      ls_model_name: 'gpt-4o',
      ls_model_type: 'chat',
      ls_temperature: 1,
      ls_max_tokens: undefined,
      ls_stop: undefined
    },
    data: { chunk: [AIMessageChunk] }
  }
]

```

```
events.slice(-2);

```

```
[
  {
    event: 'on_chat_model_stream',
    run_id: 'c983e634-9f1d-4916-97d8-63c3a86102c2',
    name: 'ChatOpenAI',
    tags: [],
    metadata: {
      ls_provider: 'openai',
      ls_model_name: 'gpt-4o',
      ls_model_type: 'chat',
      ls_temperature: 1,
      ls_max_tokens: undefined,
      ls_stop: undefined
    },
    data: { chunk: [AIMessageChunk] }
  },
  {
    event: 'on_chat_model_end',
    data: { output: [AIMessageChunk] },
    run_id: 'c983e634-9f1d-4916-97d8-63c3a86102c2',
    name: 'ChatOpenAI',
    tags: [],
    metadata: {
      ls_provider: 'openai',
      ls_model_name: 'gpt-4o',
      ls_model_type: 'chat',
      ls_temperature: 1,
      ls_max_tokens: undefined,
      ls_stop: undefined
    }
  }
]

```

### Chain[‚Äã](#chain "Direct link to Chain")

Let‚Äôs revisit the example chain that parsed streaming JSON to explore the streaming events API.

```
const chain = model.pipe(new JsonOutputParser());
const eventStream = await chain.streamEvents(
  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,
  { version: "v2" }
);

const events = [];
for await (const event of eventStream) {
  events.push(event);
}

console.log(events.length);

```

```
83

```

If you examine at the first few events, you‚Äôll notice that there are**3** different start events rather than **2** start events.

The three start events correspond to:

1. The chain (model + parser)
2. The model
3. The parser

```
events.slice(0, 3);

```

```
[
  {
    event: 'on_chain_start',
    data: {
      input: 'Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"'
    },
    name: 'RunnableSequence',
    tags: [],
    run_id: '5dd960b8-4341-4401-8993-7d04d49fcc08',
    metadata: {}
  },
  {
    event: 'on_chat_model_start',
    data: { input: [Object] },
    name: 'ChatOpenAI',
    tags: [ 'seq:step:1' ],
    run_id: '5d2917b1-886a-47a1-807d-8a0ba4cb4f65',
    metadata: {
      ls_provider: 'openai',
      ls_model_name: 'gpt-4o',
      ls_model_type: 'chat',
      ls_temperature: 1,
      ls_max_tokens: undefined,
      ls_stop: undefined
    }
  },
  {
    event: 'on_parser_start',
    data: {},
    name: 'JsonOutputParser',
    tags: [ 'seq:step:2' ],
    run_id: '756c57d6-d455-484f-a556-79a82c4e1d40',
    metadata: {}
  }
]

```

What do you think you‚Äôd see if you looked at the last 3 events? what about the middle?

Let‚Äôs use this API to take output the stream events from the model and the parser. We‚Äôre ignoring start events, end events and events from the chain.

```
let eventCount = 0;

const eventStream = await chain.streamEvents(
  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,
  { version: "v1" }
);

for await (const event of eventStream) {
  // Truncate the output
  if (eventCount > 30) {
    continue;
  }
  const eventType = event.event;
  if (eventType === "on_llm_stream") {
    console.log(`Chat model chunk: ${event.data.chunk.message.content}`);
  } else if (eventType === "on_parser_stream") {
    console.log(`Parser chunk: ${JSON.stringify(event.data.chunk)}`);
  }
  eventCount += 1;
}

```

```
Chat model chunk:
Chat model chunk: ```
Chat model chunk: json
Chat model chunk:

Chat model chunk: {

Chat model chunk:
Chat model chunk:  "
Chat model chunk: countries
Chat model chunk: ":
Chat model chunk:  [

Chat model chunk:
Chat model chunk:  {

Chat model chunk:
Chat model chunk:  "
Chat model chunk: name
Chat model chunk: ":
Chat model chunk:  "
Chat model chunk: France
Chat model chunk: ",

Chat model chunk:
Chat model chunk:  "
Chat model chunk: population
Chat model chunk: ":
Chat model chunk:
Chat model chunk: 652
Chat model chunk: 735
Chat model chunk: 11
Chat model chunk:

```

Because both the model and the parser support streaming, we see streaming events from both components in real time! Neat! ü¶ú

### Filtering Events[‚Äã](#filtering-events "Direct link to Filtering Events")

Because this API produces so many events, it is useful to be able to filter on events.

You can filter by either component `name`, component `tags` or component`type`.

#### By Name[‚Äã](#by-name "Direct link to By Name")

```
const chain = model
  .withConfig({ runName: "model" })
  .pipe(new JsonOutputParser().withConfig({ runName: "my_parser" }));

const eventStream = await chain.streamEvents(
  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,
  { version: "v2" },
  { includeNames: ["my_parser"] }
);

let eventCount = 0;

for await (const event of eventStream) {
  // Truncate the output
  if (eventCount > 10) {
    continue;
  }
  console.log(event);
  eventCount += 1;
}

```

```
{
  event: 'on_parser_start',
  data: {
    input: 'Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"'
  },
  name: 'my_parser',
  tags: [ 'seq:step:2' ],
  run_id: '0a605976-a8f8-4259-8ef6-b3d7e52b3d4e',
  metadata: {}
}
{
  event: 'on_parser_stream',
  run_id: '0a605976-a8f8-4259-8ef6-b3d7e52b3d4e',
  name: 'my_parser',
  tags: [ 'seq:step:2' ],
  metadata: {},
  data: { chunk: { countries: [Array] } }
}
{
  event: 'on_parser_end',
  data: { output: { countries: [Array] } },
  run_id: '0a605976-a8f8-4259-8ef6-b3d7e52b3d4e',
  name: 'my_parser',
  tags: [ 'seq:step:2' ],
  metadata: {}
}

```

#### By type[‚Äã](#by-type "Direct link to By type")

```
const chain = model
  .withConfig({ runName: "model" })
  .pipe(new JsonOutputParser().withConfig({ runName: "my_parser" }));

const eventStream = await chain.streamEvents(
  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,
  { version: "v2" },
  { includeTypes: ["chat_model"] }
);

let eventCount = 0;

for await (const event of eventStream) {
  // Truncate the output
  if (eventCount > 10) {
    continue;
  }
  console.log(event);
  eventCount += 1;
}

```

```
{
  event: 'on_chat_model_start',
  data: {
    input: 'Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"'
  },
  name: 'model',
  tags: [ 'seq:step:1' ],
  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: '',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',
  name: 'model',
  tags: [ 'seq:step:1' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: '```',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',
  name: 'model',
  tags: [ 'seq:step:1' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: 'json',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',
  name: 'model',
  tags: [ 'seq:step:1' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: '\n',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',
  name: 'model',
  tags: [ 'seq:step:1' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: '{\n',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',
  name: 'model',
  tags: [ 'seq:step:1' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: ' ',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',
  name: 'model',
  tags: [ 'seq:step:1' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: ' "',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',
  name: 'model',
  tags: [ 'seq:step:1' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: 'countries',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',
  name: 'model',
  tags: [ 'seq:step:1' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: '":',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',
  name: 'model',
  tags: [ 'seq:step:1' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: ' [\n',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO98p55iuqUNwx4GZ6j2BkDak6Rr',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'fb6351eb-9537-445d-a1bd-24c2e11efd8e',
  name: 'model',
  tags: [ 'seq:step:1' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}

```

#### By Tags[‚Äã](#by-tags "Direct link to By Tags")

caution

Tags are inherited by child components of a given runnable.

If you‚Äôre using tags to filter, make sure that this is what you want.

```
const chain = model
  .pipe(new JsonOutputParser().withConfig({ runName: "my_parser" }))
  .withConfig({ tags: ["my_chain"] });

const eventStream = await chain.streamEvents(
  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,
  { version: "v2" },
  { includeTags: ["my_chain"] }
);

let eventCount = 0;

for await (const event of eventStream) {
  // Truncate the output
  if (eventCount > 10) {
    continue;
  }
  console.log(event);
  eventCount += 1;
}

```

```
{
  event: 'on_chain_start',
  data: {
    input: 'Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"'
  },
  name: 'RunnableSequence',
  tags: [ 'my_chain' ],
  run_id: '1fed60d6-e0b7-4d5e-8ec7-cd7d3ee5c69f',
  metadata: {}
}
{
  event: 'on_chat_model_start',
  data: { input: { messages: [Array] } },
  name: 'ChatOpenAI',
  tags: [ 'seq:step:1', 'my_chain' ],
  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_parser_start',
  data: {},
  name: 'my_parser',
  tags: [ 'seq:step:2', 'my_chain' ],
  run_id: 'caf24a1e-255c-4937-9f38-6e46275d854a',
  metadata: {}
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: '',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',
  name: 'ChatOpenAI',
  tags: [ 'seq:step:1', 'my_chain' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: 'Certainly',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',
  name: 'ChatOpenAI',
  tags: [ 'seq:step:1', 'my_chain' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: '!',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',
  name: 'ChatOpenAI',
  tags: [ 'seq:step:1', 'my_chain' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: " Here's",
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',
  name: 'ChatOpenAI',
  tags: [ 'seq:step:1', 'my_chain' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: ' the',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',
  name: 'ChatOpenAI',
  tags: [ 'seq:step:1', 'my_chain' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: ' JSON',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',
  name: 'ChatOpenAI',
  tags: [ 'seq:step:1', 'my_chain' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: ' format',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',
  name: 'ChatOpenAI',
  tags: [ 'seq:step:1', 'my_chain' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}
{
  event: 'on_chat_model_stream',
  data: {
    chunk: AIMessageChunk {
      lc_serializable: true,
      lc_kwargs: [Object],
      lc_namespace: [Array],
      content: ' output',
      name: undefined,
      additional_kwargs: {},
      response_metadata: [Object],
      id: 'chatcmpl-9lO99nzUvCsZWCiq6vNtS1Soa1qNp',
      tool_calls: [],
      invalid_tool_calls: [],
      tool_call_chunks: [],
      usage_metadata: undefined
    }
  },
  run_id: 'ecb99d6e-ce03-445f-aadf-73e6cbbc52fe',
  name: 'ChatOpenAI',
  tags: [ 'seq:step:1', 'my_chain' ],
  metadata: {
    ls_provider: 'openai',
    ls_model_name: 'gpt-4o',
    ls_model_type: 'chat',
    ls_temperature: 1,
    ls_max_tokens: undefined,
    ls_stop: undefined
  }
}

```

### Streaming events over HTTP[‚Äã](#streaming-events-over-http "Direct link to Streaming events over HTTP")

For convenience, `streamEvents` supports encoding streamed intermediate events as HTTP [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent%5Fevents), encoded as bytes. Here‚Äôs what that looks like (using a[TextDecoder](https://developer.mozilla.org/en-US/docs/Web/API/TextDecoder)to reconvert the binary data back into a human readable string):

```
const chain = model
  .pipe(new JsonOutputParser().withConfig({ runName: "my_parser" }))
  .withConfig({ tags: ["my_chain"] });

const eventStream = await chain.streamEvents(
  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,
  {
    version: "v2",
    encoding: "text/event-stream",
  }
);

let eventCount = 0;

const textDecoder = new TextDecoder();

for await (const event of eventStream) {
  // Truncate the output
  if (eventCount > 3) {
    continue;
  }
  console.log(textDecoder.decode(event));
  eventCount += 1;
}

```

```
event: data
data: {"event":"on_chain_start","data":{"input":"Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\""},"name":"RunnableSequence","tags":["my_chain"],"run_id":"41cd92f8-9b8c-4365-8aa0-fda3abdae03d","metadata":{}}


event: data
data: {"event":"on_chat_model_start","data":{"input":{"messages":[[{"lc":1,"type":"constructor","id":["langchain_core","messages","HumanMessage"],"kwargs":{"content":"Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"","additional_kwargs":{},"response_metadata":{}}}]]}},"name":"ChatOpenAI","tags":["seq:step:1","my_chain"],"run_id":"a6c2bc61-c868-4570-a143-164e64529ee0","metadata":{"ls_provider":"openai","ls_model_name":"gpt-4o","ls_model_type":"chat","ls_temperature":1}}


event: data
data: {"event":"on_parser_start","data":{},"name":"my_parser","tags":["seq:step:2","my_chain"],"run_id":"402533c5-0e4e-425d-a556-c30a350972d0","metadata":{}}


event: data
data: {"event":"on_chat_model_stream","data":{"chunk":{"lc":1,"type":"constructor","id":["langchain_core","messages","AIMessageChunk"],"kwargs":{"content":"","tool_call_chunks":[],"additional_kwargs":{},"id":"chatcmpl-9lO9BAQwbKDy2Ou2RNFUVi0VunAsL","tool_calls":[],"invalid_tool_calls":[],"response_metadata":{"prompt":0,"completion":0,"finish_reason":null}}}},"run_id":"a6c2bc61-c868-4570-a143-164e64529ee0","name":"ChatOpenAI","tags":["seq:step:1","my_chain"],"metadata":{"ls_provider":"openai","ls_model_name":"gpt-4o","ls_model_type":"chat","ls_temperature":1}}


```

A nice feature of this format is that you can pass the resulting stream directly into a native [HTTP response object](https://developer.mozilla.org/en-US/docs/Web/API/Response) with the correct headers (commonly used by frameworks like[Hono](https://hono.dev/) and [Next.js](https://nextjs.org/)), then parse that stream on the frontend. Your server-side handler would look something like this:

```
const handler = async () => {
  const eventStream = await chain.streamEvents(
    `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`,
    {
      version: "v2",
      encoding: "text/event-stream",
    }
  );
  return new Response(eventStream, {
    headers: {
      "content-type": "text/event-stream",
    },
  });
};

```

And your frontend could look like this (using the[@microsoft/fetch-event-source](https://www.npmjs.com/package/@microsoft/fetch-event-source)pacakge to fetch and parse the event source):

```
import { fetchEventSource } from "@microsoft/fetch-event-source";

const makeChainRequest = async () => {
  await fetchEventSource("https://your_url_here", {
    method: "POST",
    body: JSON.stringify({
      foo: "bar",
    }),
    onmessage: (message) => {
      if (message.event === "data") {
        console.log(message.data);
      }
    },
    onerror: (err) => {
      console.log(err);
    },
  });
};

```

### Non-streaming components[‚Äã](#non-streaming-components-1 "Direct link to Non-streaming components")

Remember how some components don‚Äôt stream well because they don‚Äôt operate on **input streams**?

While such components can break streaming of the final output when using`stream`, `streamEvents` will still yield streaming events from intermediate steps that support streaming!

```
// A function that operates on finalized inputs
// rather than on an input_stream
import { JsonOutputParser } from "@langchain/core/output_parsers";
import { RunnablePassthrough } from "@langchain/core/runnables";

// A function that does not operates on input streams and breaks streaming.
const extractCountryNames = (inputs: Record<string, any>) => {
  if (!Array.isArray(inputs.countries)) {
    return "";
  }
  return JSON.stringify(inputs.countries.map((country) => country.name));
};

const chain = model.pipe(new JsonOutputParser()).pipe(extractCountryNames);

const stream = await chain.stream(
  `output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of "countries" which contains a list of countries. Each country should have the key "name" and "population"`
);

for await (const chunk of stream) {
  console.log(chunk);
}

```

```
["France","Spain","Japan"]

```

As expected, the `stream` API doesn‚Äôt work correctly because`extractCountryNames` doesn‚Äôt operate on streams.

Now, let‚Äôs confirm that with `streamEvents` we‚Äôre still seeing streaming output from the model and the parser.

```
const eventStream = await chain.streamEvents(
  `output a list of the countries france, spain and japan and their populations in JSON format.
Use a dict with an outer key of "countries" which contains a list of countries.
Each country should have the key "name" and "population"
Your output should ONLY contain valid JSON data. Do not include any other text or content in your output.`,
  { version: "v2" }
);

let eventCount = 0;

for await (const event of eventStream) {
  // Truncate the output
  if (eventCount > 30) {
    continue;
  }
  const eventType = event.event;
  if (eventType === "on_chat_model_stream") {
    console.log(`Chat model chunk: ${event.data.chunk.message.content}`);
  } else if (eventType === "on_parser_stream") {
    console.log(`Parser chunk: ${JSON.stringify(event.data.chunk)}`);
  } else {
    console.log(eventType);
  }
  eventCount += 1;
}

```

Chat model chunk: Chat model chunk: Here‚Äôs Chat model chunk: how Chat model chunk: you Chat model chunk: can Chat model chunk: represent Chat model chunk: the Chat model chunk: countries Chat model chunk: France Chat model chunk: , Chat model chunk: Spain Chat model chunk: , Chat model chunk: and Chat model chunk: Japan Chat model chunk: , Chat model chunk: along Chat model chunk: with Chat model chunk: their Chat model chunk: populations Chat model chunk: , Chat model chunk: in Chat model chunk: JSON Chat model chunk: format Chat model chunk: :

Chat model chunk: \`\`\` Chat model chunk: json Chat model chunk:

Chat model chunk: {

## Related[‚Äã](#related "Direct link to Related")

* [Dispatching custom events](/docs/how%5Fto/callbacks%5Fcustom%5Fevents)

---

#### Was this page helpful?

  
#### You can also leave detailed feedback [on GitHub](https://github.com/langchain-ai/langchainjs/issues/new?assignees=&labels=03+-+Documentation&projects=&template=documentation.yml&title=DOC%3A+%3CPlease+write+a+comprehensive+title+after+the+%27DOC%3A+%27+prefix%3E).

[PreviousHow to stream structured output to the client](/docs/how%5Fto/stream%5Ftool%5Fclient)[NextHow to create a time-weighted retriever](/docs/how%5Fto/time%5Fweighted%5Fvectorstore)

* [LLMs and Chat Models](#llms-and-chat-models)
* [Chains](#chains)  
   * [Working with Input Streams](#working-with-input-streams)  
   * [Non-streaming components](#non-streaming-components)
* [Using Stream Events](#using-stream-events)  
   * [Event Reference](#event-reference)  
   * [Chat Model](#chat-model)  
   * [Chain](#chain)  
   * [Filtering Events](#filtering-events)  
   * [Streaming events over HTTP](#streaming-events-over-http)  
   * [Non-streaming components](#non-streaming-components-1)
* [Related](#related)

Community

* [Twitter](https://twitter.com/LangChainAI)

GitHub

* [Python](https://github.com/langchain-ai/langchain)
* [JS/TS](https://github.com/langchain-ai/langchainjs)

More

* [Homepage](https://langchain.com)
* [Blog](https://blog.langchain.dev)

Copyright ¬© 2025 LangChain, Inc.